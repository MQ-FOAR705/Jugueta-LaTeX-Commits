\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\title{FOAR705 - Learning Journal From Week 8 to 13}
\author{Jan Jugueta - 44828020}
\date{September, October \& November 2019}

\begin{document}

\maketitle

\tableofcontents

\newpage
\section{OpenRefine Lessons}

\subsection{Creating a new OpenRefine project}

\textbf{9/9/19 - 2:01pm}

\textbf{Objective:} Create a new project

\textbf{Action:}

\begin{enumerate}
    \item Clicked on create project
    \item Selected get data from 'This Computer'
    \item Clicked on Choose Files
    \item Selected SAFI\_openrefine.csv
    \item Clicked Next
    \item Clicked Create Project
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} New project created with OpenRefine using SAFI\_openrefine.csv.

\subsection{Using Facets}

\textbf{9/9/19 - 2:11pm}

\textbf{Objective:} Use faceting to look for potential errors in data entry in the village column.

\textbf{Action:}

\begin{enumerate}
    \item Selected Text Facet in the Facet in the drop down menu from the village column
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} The facet window on the left shows all the data entries for the village column. With this view we can see the errors that have been made. Chirdozo is likely a typo. Ruca is like a typo as well. There are many entries for Ruaca - Nhamuenda. 49 is almost certainly an error, but I have no idea what it refers to so there's not much I can do about that one.

\subsection{Using clustering to detect possible typing errors}

\textbf{9/9/19 - 2:35pm}

\textbf{Objective:} To merge clusters to clean up the values in the village column

\textbf{Action:}

\begin{enumerate}
    \item Click Cluster in the village Text Facet
    \item Select the key collision method and the metaphone3 keying function
    \item Click the Merge? box beside the clusters
    \item Click Merge Selected and Recluster
    \item Tried all other Methods and Keying Functions
    \item Go back to the village Text Facet
    \item Hovered over Chirdozo and selected edit
    \item Renamed Chirdozo to Chirodzo
    \item Hovered over Ruaca-Nhamuenda and selected edit
    \item Renamed it to Ruaca.
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} There are now only 4 different values in the village column. Chirodzo, God, Ruaca and 49. Another point was that the other methods and keying functions did not find any more clusters.

\subsection{Transforming data}

\textbf{9/9/19 - 2:46pm}

\textbf{Objective:} Remove the left square brackets from the data in the items\_owned column.

\textbf{Action:}

\begin{enumerate}
    \item Clicked on the arrow at the top of the items\_owned column
    \item Select Edit Cells
    \item Select Transform...
    \item Type in \begin{verbatim}
        value.replace("[", "")]
    \end{verbatim}
    \item Click OK
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} All left brackets have been removed.

\subsection{Trim Leading and Trailing Whitespace}

\textbf{9/9/19 - 3:12pm}

\textbf{Objective:} Tidy up respondent\_wall\_type so that burntbricks and muddaub are just one value instead of two.

\textbf{Action:}

\begin{enumerate}
    \item Selected Edit cells
    \item Selected Common transforms
    \item Selected Trim leading and trailing whitespace
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} There are only four choices left in the text facet.

\subsection{Numbers}

\textbf{22/9/19 - 8:55pm}

\textbf{Objective:} Convert years\_farm column to numbers.

\textbf{Action:}

\begin{enumerate}
    \item Click the drop down arrow in the years\_farm column
    \item Click Edit cells
    \item Click Common transforms...
    \item Click To number
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} years\_farm column data converted to numbers.

\subsection{Saving your work as a script}

\textbf{22/9/19 - 9:11pm}

\textbf{Objective:} Save script in JSON format as .txt file

\textbf{Action:}

\begin{enumerate}
    \item Click on Undo / Redo
    \item Click Extract
    \item Copy the code from the window on the right hand side
    \item Paste it TextEdit
    \item Save as Plain text file
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Script saved in JSON format as .txt file.

\subsection{Importing a script to use against another data set}

\textbf{22/9/19 - 9:14pm}

\textbf{Objective:} Import script to another data set

\textbf{Action:}

\begin{enumerate}
    \item Started a new OpenRefine project
    \item Click on Undo / Redo
    \item Click on Apply
    \item Paste the contents of the JSON .txt file
    \item Click on Perform operations
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Script applied to new project. Data set has been cleaned.

\subsection{Exporting}

\textbf{22/9/19 - 9:22pm}

\textbf{Objective:} Export OpenRefine project

\textbf{Action:}

\begin{enumerate}
    \item Click Export
    \item Click Export project
    \item Download tar.gz file
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Successfully exported OpenRefine project.

Answering the additional questions. There is a history folder with with change.txt files that contain records of each individual change in the data. There is also a data.zip file which contains all the data.

\subsection{Exporting Cleaned Data}

\textbf{22/9/19 - 9:27pm}

\textbf{Objective:} Export just the cleaned data

\textbf{Action:}

\begin{enumerate}
    \item Click on Export
    \item Select Comma-separated values
    \item Download .csv file
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Successfully exported cleaned data.

\subsection{Using online resources to get help with OpenRefine}

I visited the OpenRefine documentation wiki website. I found it has useful step-by-step guides on how to perform tasks. Like other wiki sites, it is relatively easily to follow. Finding the right areas was a breeze with how it has been categorised.

\newpage
\section{OpenRefine Exercises}

\subsection{Using Facets}

\textbf{9/9/19 - 2:17pm}

\begin{enumerate}
    \item There are 19 different interview\_date values
    \item It appears as it is formatted by Text.
    \item \textbf{Objective:} Change format to Date.
    
    \textbf{Action:}
    \begin{enumerate}
        \item Clicked the drop down menu in the interview\_date column
        \item Selected Edit cells
        \item Selected Common transforms
        \item Selected To date
    \end{enumerate}
    
    \textbf{Error:} None
    
    \textbf{Result:} Data format has been changed to date. 2016-11-16T00:00:00Z is an example of the format.
    \item Most of the data was collected in November 2016.
\end{enumerate}

\subsection{Transforming data}

\textbf{9/9/19 - 2:50pm}

\textbf{Objective:} Remove the right square brackets, single quote marks and spaces from the data in the items\_owned column.

\textbf{Action:}

\begin{enumerate}
    \item Clicked on the arrow at the top of the items\_owned column
    \item Select Edit Cells
    \item Select Transform...
    \item Type in \begin{verbatim}
        value.replace("'", "")
        value.replace("]", "")
        value.replace(" ", "")
    \end{verbatim}
    \item Click OK
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} All right brackets, single quote marks and spaces have been removed.

\textbf{9/9/19 - 2:54pm}

By sorting with count we can see which are the most commonly owned items. They are the mobile phone and radio. The least commonly owned items are cars and computers.

\textbf{9/9/19 - 2:57pm}

\textbf{Objective:} Finding which month(s) farmers were more likely to lack food

\textbf{Action:}

\begin{enumerate}
    \item Select Transform... in the Edit cells drop down menu in the months\_lack\_food column.
    \item Enter expression \begin{verbatim}
        value.replace("[", "").replace("]", "").replace(" ", "").replace("'", "")
    \end{verbatim}
    \item Create Custom text facet for months\_lack\_food column.
    \item Enter expression \begin{verbatim}
        value.split(";")
    \end{verbatim}
    \item Sort by count
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} November was the most common month that farmers lacked food.

\textbf{9/9/19 - 3:04pm}

\textbf{Objective:} Clean up months\_no\_water, liv\_owned, res\_change, and no\_food\_mitigation columns.

\textbf{Action:}

\begin{enumerate}
    \item Clicked on months\_no\_water column
    \item Selected Transform...
    \item Went to the History tab
    \item Reused the last expression in history
    \item Repeated the process for liv\_owned, res\_change, and no\_food\_mitigation columns
\end{enumerate}

\textbf{Error:} None

\textbf{Result:} Cleaned up months\_no\_water, liv\_owned, res\_change, and no\_food\_mitigation columns.

\subsection{Using undo and redo}

\textbf{9/9/19 - 3:09pm}

The undo and redo functions work just like explained in the lesson.

\subsection{Filtering}

\textbf{10/9/19 - 4:48pm}

\begin{enumerate}
    \item The roof types returned are mabatipitched and mabatisloping
    \item To restrict it further, you could be more specific with the spelling.
\end{enumerate}

\subsection{Excluding entries}

\textbf{10/9/19 - 5:05pm}

Have played with the include / exclude option. A reminder that this option appears in the facet, not the filter.

\subsection{Sort}

\textbf{10/9/19 - 5:10pm}

\textbf{Objective:} Find out if there are incorrect altitudes in the gps\_Altitude column.

\textbf{Action:}

\begin{enumerate}
    \item Go to the gps\_Altitude column
    \item Select sort in the triangle drop down menu
    \item Select Sort
    \item Select numbers
    \item Select smallest first
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} There are a few 0's. This is most likely missing data that was not recorded.

\subsection{Sorting by multiple columns}

\textbf{10/9/19 - 5:18pm}

When sorting by GPS coordinates, village 49 seems to match with Chirodzo. When using the interview date, it also seems to suggest Chirodzo.

\subsection{Numbers}

\textbf{22/9/19 - 9:02pm}

Only numerals can be transformed into numbers.

\subsection{Numeric facets}

\textbf{22/9/19 - 9:04pm}

The blank and non-numeric facets were ticked, presumably because of the 'abc' and blank data.

\newpage
\section{OpenRefine Lesson Notes}

\begin{itemize}
    \item Data is often very messy. OpenRefine can help organise messy data.
    \item OpenRefine records all changes to the data.
    \item OpenRefine does not modify the original dataset.
    \item OpenRefine is open source.
    \item Facets help get an overview of the data as well as bring consistency to the data.
\end{itemize}

\section{OpenRefine Error List}

No errors occured when completing the OpenRefine exercises and lessons.

\newpage
\section{R for Social Scientists Lessons}

\subsection{Create a new project}

\textbf{9/10/19 - 1:24pm}

\textbf{Objective:} Create a new project in R Studio

\textbf{Action:}

\begin{itemize}
    \item Open RStudio
    \item Click File - New project - New directory - New project
    \item Enter the name data-carpentry
\item Click Create project
\item Click File - New File - R script
\item Click save icon and name script "script.R"
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} New data-carpentry project created along with script.R

\subsection{Downloading the data and getting set up}

\textbf{9/10/19 - 1:34pm}

\textbf{Objective:} Create the directories "data", "data\_output" and "fig\_output"

\textbf{Action:} Type in the code
\begin{verbatim}
    dir.create("data")
    dir.create("data_output")
    dir.create("fig_output")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} The three directories were created.

\subsection{Presentation of the SAFI data}

\textbf{13/10/19 - 11:44am}

\textbf{Objective:} Load tidyverse and make an interviews object

\textbf{Action:} Used the following codes \begin{verbatim}
    library(tidyverse)
    interviews <- read_csv("data/SAFI_clean.csv", na = "NULL")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Tidyverse loaded and created the interviews object.

\subsection{Indexing and subsetting data frames}

\textbf{13/10/19 - 11:57am}

Just following with the lesson. I used the following codes in this section.

\begin{verbatim}
    ## first element in the first column of the data frame (as a vector)
    interviews[1, 1]
    ## first element in the 6th column (as a vector)
    interviews[1, 6]
    ## first column of the data frame (as a vector)
    interviews[[1]]
    ## first column of the data frame (as a data.frame)
    interviews[1]
    ## first three elements in the 7th column (as a vector)
    interviews[1:3, 7]
    ## the 3rd row of the data frame (as a data.frame)
    interviews[3, ]
    ## equivalent to head_interviews <- head(interviews)
    head_interviews <- interviews[1:6, ]
    interviews[, -1]          # The whole data frame, except the first column
    interviews[-c(7:131), ]   # Equivalent to head(interviews)
    interviews["village"]       # Result is a data frame
    interviews[, "village"]     # Result is a data frame
    interviews[["village"]]     # Result is a vector
    interviews$village          # Result is a vector
\end{verbatim}

\subsection{Factors}

\textbf{13/10/19 - 12:04pm}

I just followed along with the lesson and had no errors. Below is the list of code I inputted into RStudio:
\begin{verbatim}
    respondent_floor_type <- factor(c("earth", "cement", "cement", "earth"))
    levels(respondent_floor_type)
    nlevels(respondent_floor_type)
    respondent_floor_type # current order
    respondent_floor_type <- factor(respondent_floor_type, levels = c("earth", "cement"))
    respondent_floor_type # after re-ordering
    levels(respondent_floor_type)
    levels(respondent_floor_type)[2] <- "brick"
    levels(respondent_floor_type)
    respondent_floor_type
\end{verbatim}

\subsection{Converting factors}

\textbf{13/10/19 - 12:09pm}

\begin{verbatim}
    as.character(respondent_floor_type)
    year_fct <- factor(c(1990, 1983, 1977, 1998, 1990))
    as.numeric(year_fct)                     # Wrong! And there is no warning...
    as.numeric(as.character(year_fct))       # Works...
    as.numeric(levels(year_fct))[year_fct]   # The recommended way.
\end{verbatim}

\subsection{Renaming factors}

\textbf{13/10/19 - 12:15pm}

SO I have kept on following the lesson. I used these codes: \begin{verbatim}
    ## create a vector from the data frame column "memb_assoc"
    memb_assoc <- interviews$memb_assoc
    ## convert it into a factor
    memb_assoc <- as.factor(memb_assoc)
    ## let's see what it looks like
    memb_assoc
    ## bar plot of the number of interview respondents who were
    ## members of irrigation association:
    plot(memb_assoc)
\end{verbatim}

I noticed that a plot appeared in one of the panes. I also noticed I could export it, so I decided to do that. I set the export path to point to fig\_output directory. This is what the image looks like.

\includegraphics[width=\textwidth]{Rplot.png}

Continuing with the lesson, I entered these codes: \begin{verbatim}
    memb_assoc <- interviews$memb_assoc
    memb_assoc[is.na(memb_assoc)] <- "undetermined"
    memb_assoc <- as.factor(memb_assoc)
    memb_assoc
    plot(memb_assoc)
\end{verbatim}

And got this plot.

\includegraphics[width=\textwidth]{Rplot01.png}

\subsection{Formatting dates}

\textbf{13/10/19 - 12:33pm}

Following on with the lesson. Experienced no errors when inputting these codes: \begin{verbatim}
    str(interviews)
    library(lubridate)
    dates <- interviews$interview_date
    str(dates)
    interviews$day <- day(dates)
    interviews$month <- month(dates)
    interviews$year <- year(dates)
    interviews
\end{verbatim}

\subsection{Learning dplyr and tidyr}

\textbf{22/10/19 - 5:49pm}

Reloaded tidyverse and reviewed the data.

\subsection{Selecting columns and filtering rows}

\textbf{22/10/19 - 5:52pm}

Inputed \begin{verbatim}
    select(interviews, village, no_membrs, years_liv)
\end{verbatim}

Returned \begin{verbatim}
    # A tibble: 131 x 3
   village  no_membrs years_liv
   <chr>        <dbl>     <dbl>
 1 God              3         4
 2 God              7         9
 3 God             10        15
 4 God              7         6
 5 God              7        40
 6 God              3         3
 7 God              6        38
 8 Chirodzo        12        70
 9 Chirodzo         8         6
10 Chirodzo        12        23
# … with 121 more rows
\end{verbatim}

Filtered for "God" with \begin{verbatim}
    filter(interviews, village == "God")
\end{verbatim}

Returned \begin{verbatim}
    # A tibble: 43 x 17
   key_ID village interview_date      no_membrs years_liv respondent_wall… rooms
    <dbl> <chr>   <dttm>                  <dbl>     <dbl> <chr>            <dbl>
 1      1 God     2016-11-17 00:00:00         3         4 muddaub              1
 2      1 God     2016-11-17 00:00:00         7         9 muddaub              1
 3      3 God     2016-11-17 00:00:00        10        15 burntbricks          1
 4      4 God     2016-11-17 00:00:00         7         6 burntbricks          1
 5      5 God     2016-11-17 00:00:00         7        40 burntbricks          1
 6      6 God     2016-11-17 00:00:00         3         3 muddaub              1
 7      7 God     2016-11-17 00:00:00         6        38 muddaub              1
 8     11 God     2016-11-21 00:00:00         6        20 sunbricks            1
 9     12 God     2016-11-21 00:00:00         7        20 burntbricks          3
10     13 God     2016-11-21 00:00:00         6         8 burntbricks          1
# … with 33 more rows, and 10 more variables: memb_assoc <chr>,
#   affect_conflicts <chr>, liv_count <dbl>, items_owned <chr>, no_meals <dbl>,
#   months_lack_food <chr>, instanceID <chr>, day <int>, month <dbl>, year <dbl>
\end{verbatim}

Not sure why my came back with 10 more variables as opposed to 8.

\subsection{Pipes}

\textbf{22/10/19 - 6:04pm}

Tried my hand at pipes in R using this \begin{verbatim}
    interviews %>%
    filter(village == "God") %>%
    select(no_membrs, years_liv)
\end{verbatim}

And it came back with this \begin{verbatim}
    # A tibble: 43 x 2
   no_membrs years_liv
       <dbl>     <dbl>
 1         3         4
 2         7         9
 3        10        15
 4         7         6
 5         7        40
 6         3         3
 7         6        38
 8         6        20
 9         7        20
10         6         8
# … with 33 more rows
\end{verbatim}

which was the same as the lesson. So good.

\textbf{22/10/19 - 6:20pm}

Keeping on with pipes. I put this in \begin{verbatim}
    interviews_god <- interviews %>%
    filter(village == "God") %>%
    select(no_membrs, years_liv

    interviews_god
\end{verbatim}

and got this out \begin{verbatim}
    # A tibble: 43 x 2
   no_membrs years_liv
       <dbl>     <dbl>
 1         3         4
 2         7         9
 3        10        15
 4         7         6
 5         7        40
 6         3         3
 7         6        38
 8         6        20
 9         7        20
10         6         8
# … with 33 more rows
\end{verbatim}

\subsection{Mutate}

\textbf{22/10/19 - 6:29pm}

Used this code \begin{verbatim}
    interviews %>%
    mutate(people_per_room = no_membrs / rooms)
\end{verbatim}

and had this returned, just like in the lesson. \begin{verbatim}
    # A tibble: 131 x 18
   key_ID village interview_date      no_membrs years_liv respondent_wall… rooms
    <dbl> <chr>   <dttm>                  <dbl>     <dbl> <chr>            <dbl>
 1      1 God     2016-11-17 00:00:00         3         4 muddaub              1
 2      1 God     2016-11-17 00:00:00         7         9 muddaub              1
 3      3 God     2016-11-17 00:00:00        10        15 burntbricks          1
 4      4 God     2016-11-17 00:00:00         7         6 burntbricks          1
 5      5 God     2016-11-17 00:00:00         7        40 burntbricks          1
 6      6 God     2016-11-17 00:00:00         3         3 muddaub              1
 7      7 God     2016-11-17 00:00:00         6        38 muddaub              1
 8      8 Chirod… 2016-11-16 00:00:00        12        70 burntbricks          3
 9      9 Chirod… 2016-11-16 00:00:00         8         6 burntbricks          1
10     10 Chirod… 2016-12-16 00:00:00        12        23 burntbricks          5
# … with 121 more rows, and 11 more variables: memb_assoc <chr>,
#   affect_conflicts <chr>, liv_count <dbl>, items_owned <chr>, no_meals <dbl>,
#   months_lack_food <chr>, instanceID <chr>, day <int>, month <dbl>, year <dbl>,
#   people_per_room <dbl>
\end{verbatim}

\textbf{22/10/19 - 6:32pm}

Now using a filter to remove NULL data

\begin{verbatim}
    interviews %>%
    filter(!is.na(memb_assoc)) %>%
    mutate(people_per_room = no_membrs / rooms)
\end{verbatim}

which returned \begin{verbatim}
    # A tibble: 92 x 18
   key_ID village interview_date      no_membrs years_liv respondent_wall… rooms
    <dbl> <chr>   <dttm>                  <dbl>     <dbl> <chr>            <dbl>
 1      1 God     2016-11-17 00:00:00         7         9 muddaub              1
 2      7 God     2016-11-17 00:00:00         6        38 muddaub              1
 3      8 Chirod… 2016-11-16 00:00:00        12        70 burntbricks          3
 4      9 Chirod… 2016-11-16 00:00:00         8         6 burntbricks          1
 5     10 Chirod… 2016-12-16 00:00:00        12        23 burntbricks          5
 6     12 God     2016-11-21 00:00:00         7        20 burntbricks          3
 7     13 God     2016-11-21 00:00:00         6         8 burntbricks          1
 8     15 God     2016-11-21 00:00:00         5        30 sunbricks            2
 9     21 God     2016-11-21 00:00:00         8        20 burntbricks          1
10     24 Ruaca   2016-11-21 00:00:00         6         4 burntbricks          2
# … with 82 more rows, and 11 more variables: memb_assoc <chr>,
#   affect_conflicts <chr>, liv_count <dbl>, items_owned <chr>, no_meals <dbl>,
#   months_lack_food <chr>, instanceID <chr>, day <int>, month <dbl>, year <dbl>,
#   people_per_room <dbl>
\end{verbatim}

! negates the result, so we are asking when it is NOT NULL.

\subsection{Split-apply-combine data analysis and the summarize() function}

\subsubsection{The summarize() function}

\textbf{22/10/19 - 7:01pm}

Using group by and summarize functions.

\begin{verbatim}
    interviews %>%
    group_by(village) %>%
    summarize(mean_no_membrs = mean(no_membrs))
\end{verbatim}

R returned \begin{verbatim}
    # A tibble: 3 x 2
  village  mean_no_membrs
  <chr>             <dbl>
1 Chirodzo           7.08
2 God                6.86
3 Ruaca              7.57
\end{verbatim}

\textbf{22/10/19 - 7:59pm}

Grouping by multiple columns.

Used this code \begin{verbatim}
    interviews %>%
    group_by(village, memb_assoc) %>%
    summarize(mean_no_membrs = mean(no_membrs))
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 9 x 3
# Groups:   village [3]
  village  memb_assoc mean_no_membrs
  <chr>    <chr>               <dbl>
1 Chirodzo no                   8.06
2 Chirodzo yes                  7.82
3 Chirodzo NA                   5.08
4 God      no                   7.13
5 God      yes                  8   
6 God      NA                   6   
7 Ruaca    no                   7.18
8 Ruaca    yes                  9.5 
9 Ruaca    NA                   6.22
\end{verbatim}

\textbf{22/10/19 - 8:05pm}

Excluding data from our table using a filter step. Use the code \begin{verbatim}
    interviews %>%
    filter(!is.na(memb_assoc)) %>%
    group_by(village, memb_assoc) %>%
    summarize(mean_no_membrs = mean(no_membrs))
\end{verbatim}

Returned this \begin{verbatim}
    # A tibble: 6 x 3
# Groups:   village [3]
  village  memb_assoc mean_no_membrs
  <chr>    <chr>               <dbl>
1 Chirodzo no                   8.06
2 Chirodzo yes                  7.82
3 God      no                   7.13
4 God      yes                  8   
5 Ruaca    no                   7.18
6 Ruaca    yes                  9.5 
\end{verbatim}

\textbf{22/10/19 - 8:08pm}

Summarise multiple variables at the same time. Used the code \begin{verbatim}
    interviews %>%
    filter(!is.na(memb_assoc)) %>%
    group_by(village, memb_assoc) %>%
    summarize(mean_no_membrs = mean(no_membrs),
              min_membrs = min(no_membrs))
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 6 x 4
# Groups:   village [3]
  village  memb_assoc mean_no_membrs min_members
  <chr>    <chr>               <dbl>       <dbl>
1 Chirodzo no                   8.06           4
2 Chirodzo yes                  7.82           2
3 God      no                   7.13           3
4 God      yes                  8              5
5 Ruaca    no                   7.18           2
6 Ruaca    yes                  9.5            5
\end{verbatim}

\textbf{22/10/19 - 8:12pm}

Rearrange the result of a query to inspect the values. Used the code \begin{verbatim}
    interviews %>% 
  filter(!is.na(memb_assoc)) %>% 
  group_by(village, memb_assoc) %>% 
  summarize(mean_no_membrs = mean(no_membrs), min_membrs = min(no_membrs)) %>% 
  arrange(min_membrs)
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 6 x 4
# Groups:   village [3]
  village  memb_assoc mean_no_membrs min_membrs
  <chr>    <chr>               <dbl>      <dbl>
1 Chirodzo yes                  7.82          2
2 Ruaca    no                   7.18          2
3 God      no                   7.13          3
4 Chirodzo no                   8.06          4
5 God      yes                  8             5
6 Ruaca    yes                  9.5           5
\end{verbatim}

\textbf{22/10/19 - 8:14pm}

Sort in descending order. Used this code \begin{verbatim}
    interviews %>% 
  filter(!is.na(memb_assoc)) %>% 
  group_by(village, memb_assoc) %>% 
  summarize(mean_no_membrs = mean(no_membrs), min_membrs = min(no_membrs)) %>%
  arrange(desc(min_membrs))
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 6 x 4
# Groups:   village [3]
  village  memb_assoc mean_no_membrs min_membrs
  <chr>    <chr>               <dbl>      <dbl>
1 God      yes                  8             5
2 Ruaca    yes                  9.5           5
3 Chirodzo no                   8.06          4
4 God      no                   7.13          3
5 Chirodzo yes                  7.82          2
6 Ruaca    no                   7.18          2
\end{verbatim}

\subsubsection{Counting}

\textbf{22/10/19 - 8:17pm}

Count the number of rows of data for each village. Used the code \begin{verbatim}
    interviews %>% 
  count(village)
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 3 x 2
  village      n
  <chr>    <int>
1 Chirodzo    39
2 God         43
3 Ruaca       49
\end{verbatim}

\textbf{22/10/19 - 8:18pm}

Using 'sort' to get the results in decreasing order. Used the code \begin{verbatim}
    interviews %>% 
  count(village, sort = TRUE)
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 3 x 2
  village      n
  <chr>    <int>
1 Ruaca       49
2 God         43
3 Chirodzo    39
\end{verbatim}

\subsection{Reshaping with gather and spread}

\subsubsection{Spreading}

\textbf{22/10/19 - 8:31pm}

Used the code \begin{verbatim}
    interviews_spread <- interviews %>%
    mutate(wall_type_logical = TRUE) %>%
    spread(key = respondent_wall_type, value = wall_type_logical, fill = FALSE)
\end{verbatim}

Not sure what happened here, if anything?

\subsection{Gathering}

\textbf{22/10/19 - 8:34pm}

Used the code \begin{verbatim}
    interviews_gather <- interviews_spread %>%
    gather(key = respondent_wall_type, value = "wall_type_logical",
           burntbricks:sunbricks)
\end{verbatim}

Also used the code \begin{verbatim}
    interviews_gather <- interviews_spread %>%
    gather(key = "respondent_wall_type", value = "wall_type_logical",
           burntbricks:sunbricks) %>%
    filter(wall_type_logical) %>%
    select(-wall_type_logical)
\end{verbatim}

Again, not sure what happened here...

\subsection{Applying spread() to clean our data}

\textbf{22/10/19 - 8:40pm}

Used the code \begin{verbatim}
    interviews_items_owned <- interviews %>%
  separate_rows(items_owned, sep=";") %>%
  mutate(items_owned_logical = TRUE) %>%
  spread(key = items_owned, value = items_owned_logical, fill = FALSE)

nrow(interviews_items_owned)
\end{verbatim}

Returned with \begin{verbatim}
    [1] 131
\end{verbatim}

\textbf{22/10/19 - 8:41pm}

Entered \begin{verbatim}
    interviews_items_owned <- interviews_items_owned %>%
    rename(no_listed_items = `<NA>`)
\end{verbatim}

\textbf{22/10/19 - 8:43pm}

Show the number of respondents in each village who owned a particular item \begin{verbatim}
    interviews_items_owned %>%
  filter(bicycle) %>%
  group_by(village) %>%
  count(bicycle)
\end{verbatim}

and returned \begin{verbatim}
    # A tibble: 3 x 3
# Groups:   village [3]
  village  bicycle     n
  <chr>    <lgl>   <int>
1 Chirodzo TRUE       17
2 God      TRUE       23
3 Ruaca    TRUE       20
\end{verbatim}

\textbf{22/10/19 - 8:45pm}

Calculate the average number of items from the list owned by respondents in each village. Used the code \begin{verbatim}
    interviews_items_owned %>%
    mutate(number_items = rowSums(select(., bicycle:television))) %>%
    group_by(village) %>%
    summarize(mean_items = mean(number_items))
\end{verbatim}

which returned \begin{verbatim}
    # A tibble: 3 x 2
  village  mean_items
  <chr>         <dbl>
1 Chirodzo       4.54
2 God            3.98
3 Ruaca          5.57
\end{verbatim}

\subsection{Exporting data}

Followed the lesson and used this code \begin{verbatim}
    interviews_plotting <- interviews %>%
    ## spread data by items_owned
    separate_rows(items_owned, sep=";") %>%
    mutate(items_owned_logical = TRUE) %>%
    spread(key = items_owned, value = items_owned_logical, fill = FALSE) %>%
    rename(no_listed_items = `<NA>`) %>%
    ## spread data by months_lack_food
    separate_rows(months_lack_food, sep=";") %>%
    mutate(months_lack_food_logical = TRUE) %>%
    spread(key = months_lack_food, value = months_lack_food_logical, fill = FALSE) %>%
    ## add some summary columns
    mutate(number_months_lack_food = rowSums(select(., Apr:Sept))) %>%
    mutate(number_items = rowSums(select(., bicycle:television)))
\end{verbatim}

and

\begin{verbatim}
    write_csv(interviews_plotting, path = "data_output/interviews_plotting.csv")
\end{verbatim}

\newpage
\section{R for Social Scientists Exercises}

\subsection{Installing additional packages using the packages tab}

\textbf{9/10/19 - 1:54pm}

\textbf{Objective:} Download tidyverse package

\textbf{Action:}
\begin{itemize}
    \item Click on Packages tab
\item Click on Install
\item Type in tidyverse
\item Click Install
\end{itemize}

\textbf{Error: None}

\textbf{Result:} Tidyverse installed, along with what looks like many other packages.

\subsection{Creating ojbects in R}

\textbf{9/10/19 - 4:36pm}

The value is still 6.175

\subsection{Comments}

\textbf{9/10/19 - 4:49pm}

\textbf{Objective:} See what happens when I create objects for width and length and get them to work with area.

\textbf{Action:} Entered the following code in:
\begin{verbatim}
    length <- 2
    width <- 1
    area <- length*witdh
\end{verbatim}

Then changed the value for length to 7.0 and width to 6.5. Retyped area.

\textbf{Error:} None.

\textbf{Result:} Area did not change after the values of length and width were changed. This is presumably because \begin{verbatim}
    area <- length*width
\end{verbatim} was not run again.

\subsection{Functions and their arguments}

\textbf{9/10/19 - 5:20pm}

Using \begin{verbatim}
    ?round
\end{verbatim} it seems that there are other functions similar to it. These include \begin{verbatim}
    ceiling(x)
    floor(x)
    trunc(x,...)
    signif(x,digits=6)
\end{verbatim}

\subsection{Vectors and data types}

\textbf{9/10/19 - 5:46pm}

If we try to mix different vectors together, R will convert them all to be the same type.

For the second part of this exercise, it seems that R will find the common denominator and convert them all to that same type.

For the third part of the exercise, only one value of TRUE is present, which is part of the \begin{verbatim}
    char_logical
\end{verbatim} object.

\subsection{Missing data}

\textbf{9/10/19 - 6:15pm}

\textbf{Objective:} Create a new vector with NAs removed then use \begin{verbatim}
    median()
\end{verbatim} to find the median of the rooms.

\textbf{Action:} Used the following codes:
\begin{verbatim}
    rooms_no_na <- na.omit(rooms)
    median(rooms,na.rm=TRUE)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} The median returned the value of 1.

\textbf{9/10/19 - 6:18pm}

\textbf{Objective:} Use R to figure out how many households in the set use more than 2 rooms for sleeping

\textbf{Action:} Used the following codes \begin{verbatim}
    rooms_above_2 <- rooms_no_na[rooms_no_na > 2]
length(rooms_above_2)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Returned the value of 4.

\subsection{Indexing and subsetting data frames}

\textbf{13/10/19 - 12:01pm}

For this exercise, I used these codes in this order:

\begin{verbatim}
    interviews_100 <- interviews[100, ]
    n_rows <- nrow(interviews)
    interviews_last <- interviews[n_rows, ]
    interviews_middle <- interviews[(n_rows / 2), ]
    interviews_head <- interviews[-(7:n_rows), ]
\end{verbatim}

\subsection{Renaming factors}

\textbf{13/10/19 - 12:24pm}

\textbf{Objective:} Rename the levels of the factor to have the first letter in uppercase: "No", "Undetermined", and "Yes".

\textbf{Action:} I used the following code: \begin{verbatim}
    levels(memb_assoc) <- c("No", "Undetermined", "Yes")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} They are now in uppercase.

\textbf{13/10/19 - 12:26pm}

\textbf{Objective:} Recreate the barplot so that "Undetermined" is last.

\textbf{Action:} I used the following code: \begin{verbatim}
    memb_assoc <- factor(memb_assoc, levels = c("No", "Yes", "Undetermined"))
    plot(memb_assoc)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Please see the below plot.

\includegraphics[width=\textwidth]{Rplot02.png}

\subsection{Pipes}

\textbf{22/10/19 - 6:25pm}

\textbf{Objective:} Using pipes, subset the interviews data to include interviews where respondents were members of an irrigation association (memb\_assoc) and retain only the columns affect\_conflicts, liv\_count, and no\_meals.

\textbf{Action:} Used this code \begin{verbatim}
    interviews %>%
    filter(memb_assoc == "yes") %>%
    select(affect_conflicts, liv_count, no_meals)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} RStudio returned me this. \begin{verbatim}
    # A tibble: 33 x 3
   affect_conflicts liv_count no_meals
   <chr>                <dbl>    <dbl>
 1 once                     3        2
 2 never                    2        2
 3 never                    2        3
 4 once                     3        2
 5 frequently               1        3
 6 more_once                5        2
 7 more_once                3        2
 8 more_once                2        3
 9 once                     3        3
10 never                    3        3
# … with 23 more rows
\end{verbatim}

\subsection{Mutate}

\textbf{22/10/19 - 6:44pm}

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared.

\textbf{22/10/19 - 6:50pm}

Trying again.

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code and entered each line individually this time. \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared. I don't get it. I'm moving on.

EDIT: It did work. I typed in \begin{verbatim}
    interviews_total_meals
\end{verbatim} and it had shown that contents of the object. Therefore it was user error.

\subsection{Split-apply-combine data analysis and the summarize() function - counting}

\textbf{22/10/19 - 8:21pm}

\textbf{Objective:} Find out how many households in the survey have an average of two meals per day? Three meals per day?

\textbf{Action:} Used the code \begin{verbatim}
    interviews %>% 
  count(no_meals)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} R returned this \begin{verbatim}
    # A tibble: 2 x 2
  no_meals     n
     <dbl> <int>
1        2    52
2        3    79
\end{verbatim}

Households who have two meals a day are 52 in number. 79 have three a day.

\textbf{22/10/19 - 8:24pm}

\textbf{Objective:} Use group\_by() and summarize() to find the mean, min, and max number of household members for each village.

\textbf{Action:} Used the code \begin{verbatim}
    interviews %>%
  group_by(village) %>%
  summarize(
      mean_no_membrs = mean(no_membrs),
      min_no_membrs = min(no_membrs),
      max_no_membrs = max(no_membrs),
      n = n()
  )
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} R returned this \begin{verbatim}
    # A tibble: 3 x 5
  village  mean_no_membrs min_no_membrs max_no_membrs     n
  <chr>             <dbl>         <dbl>         <dbl> <int>
1 Chirodzo           7.08             2            12    39
2 God                6.86             3            15    43
3 Ruaca              7.57             2            19    49
\end{verbatim}

\textbf{22/10/19 - 8:26pm}

\textbf{Objective:} Find out what was the largest household interviewed in each month?

\textbf{Action:} Loaded lubridate by using the code \begin{verbatim}
    library(lubridate)
\end{verbatim}

Then used this code \begin{verbatim}
    interviews %>%
  mutate(month = month(interview_date),
         day = day(interview_date),
         year = year(interview_date)) %>%
  group_by(year, month) %>%
  summarize(max_no_membrs = max(no_membrs))
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} R returned \begin{verbatim}
    # A tibble: 5 x 3
# Groups:   year [2]
   year month max_no_membrs
  <dbl> <dbl>         <dbl>
1  2016    11            19
2  2016    12            12
3  2017     4            17
4  2017     5            15
5  2017     6            15
\end{verbatim}

\subsection{Applying spread() to clean our data}

\textbf{22/10/19 - 8:49pm}

\textbf{Objective:} Create a new data frame (named interviews\_months\_lack\_food) that has one column for each month and records TRUE or FALSE for whether each interview respondent was lacking food in that month.

\textbf{Action:} Used the code \begin{verbatim}
    interviews_months_lack_food <- interviews %>%
  separate_rows(months_lack_food, sep=";") %>%
  mutate(months_lack_food_logical  = TRUE) %>%
  spread(key = months_lack_food, value = months_lack_food_logical, fill = FALSE)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Correct.

\textbf{22/10/19 - 8:52pm}

\textbf{Objective:} Find out how many months (on average) were respondents without food if they did belong to an irrigation association?

\textbf{Action:} Used the code \begin{verbatim}
    interviews_months_lack_food %>%
  mutate(number_months = rowSums(select(., Apr:Sept))) %>%
  group_by(memb_assoc) %>%
  summarize(mean_months = mean(number_months))
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success. R returned with \begin{verbatim}
    # A tibble: 3 x 2
  memb_assoc mean_months
  <chr>            <dbl>
1 no                2.31
2 yes               2.64
3 NA                2.95
\end{verbatim}

\newpage
\section{R Error List}

\subsection{Mutate}

\textbf{22/10/19 - 6:44pm}

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared.

\textbf{22/10/19 - 6:50pm}

Trying again.

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code and entered each line individually this time. \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared. I don't get it. I'm moving on.

EDIT: It did work. I typed in \begin{verbatim}
    interviews_total_meals
\end{verbatim} and it had shown that contents of the object. Therefore it was user error.

\newpage
\section{Proof of Concept}

\subsection{General Thoughts}

\textbf{13/9/19 - 5:21pm}

I have received back my marks for the Elaboration Results and it has left me disheartened. I think now more than ever am I unsure about what I am doing, whether I should be aiming to stretch myself and try something ambitious or just design something that will work and assist me in my research. Honestly, I am leaning towards the latter.

Two main points of feedback for my Elaboration results are:
\begin{enumerate}
    \item I didn't detail alternatives to Voyant Tools
    \item That simply going through the articles I find in \textit{Neues Deutschland} and copying and pasting them into .txt files is not the best way to move forward
\end{enumerate}

I accept the feedback regarding the first point. I should have detailed alternatives to Voyant Tools for my textual analysis. As for the second, I would rather not automate that process for two main reasons.
\begin{enumerate}
    \item If I were to somehow automate the process of copying text from the \textit{Neues Deutschland} website, it would also copy in advertisements and other non-relevant text that would be too much of a hassle to clean up after the fact.
    \item Many results returned for the search query are not relevant at all to my research. This would only serve to save more unnecessary articles on my machine, which could skew corpus analysis at a later date. I would feel more comfortable actually \textbf{reading} every article on \textit{Neues Deutschland} before choosing which I should save and which I would not.
\end{enumerate}

\textbf{16/9/19 - 3:15pm}

Having a look at the next step for the Proof of Concept and I am finding it difficult ascertaining what I should actually be doing. It seems like the user stories that I have come up with are very similar, if not the same as the steps I had covered in Elaboration? Do I just repeat that work?

\textbf{16/9/19 - 4:02pm}

Without receiving feedback on my Proof of Concept Design with user stories, I am going to go ahead with what I have come up with. I am making the point that the collection of articles has already been done. My Proof of Concept will only be concerned with the analysis of text and storage of data.

\textbf{16/9/19 - 4:44pm}

Having completed learning how to save the textual analysis performed by Voyant Tools as a .tsv file, I now realise it is in a format that can work with OpenRefine. I may now consider using OpenRefine in my workflow to analyse the data once more.

\textbf{19/9/19 - 10:23am}

I think I need to read more about Open Semantic Desktop Search. It seems like a very powerful piece of software. For now, I am happy that it is able to do the tasks I need it to do, but I have no doubt that it probably can do so much more.

\textbf{9/10/19 - 10:23am}

After clearing some of my uni workload, I was finally able to organise a meeting with Brian. The meeting proved to be very helpful, with Brian providing me clarification on what I needed to do as part of my Proof of Concept.

My main take away from this was that the point of the Proof of Concept was to get the computer to do more work where it can, giving me the opportunity to drink more coffee (or another beverage of my choice). I realised my Proof of Concept was automating the process of getting text analysed using Voyant Tools and straight into Open Semantic Desktop Search.

I have downloaded the Voyant Tools open-source software directly on my machine. Let's see what I can do with this.

\textbf{10/10/19 - 10:48pm}

So I finally got around to creating a GitHub repository for my Proof of Concept in the FOAR705 organisation. I feel like I'm playing catch up, but at least I know what I'm doing now.

\textbf{10/10/19 - 11:03pm}

I want to put in writing what I am actually working on as part of my Proof of Concept. What I want to do is to automate the process of \textbf{\textit{saving}} the results of textual analysis straight into Open Semantic Desktop Search. Having that tagged automatically would be great too.

\textbf{10/10/19 - 11:21pm}

I have downloaded Duplicati because it seems like a decent enough back up solution. I will look into it much further later.

\textbf{11/10/19 - 1:44pm}

Have sat in the consultation hour before class seeing if I could figure out how to output data from Voyant. Still no idea.

\textbf{11/10/19 - 4:30pm}

Just stayed back after class to work with Aaron Hammond trying to find the elusive Voyant API. Had no luck. Instead, I made a Slack channel called \#voyanttools to pool together students who are using Voyant in their workflow.

\textbf{16/10/19 - 2:10pm}

I have met with fellow student Sophie Avard to see where we align in our Proof of Concepts. We have both tried to find the Voyant API. We both have had no luck with this. I'm starting to wonder if it exists? Some documentation on GitHub keep pointing to "Trombone" as a backend into Voyant, but I still don't know how this works.

\textbf{17/10/19 - 6:40pm}

A few things have transpired since the last update.
\begin{itemize}
    \item First, and perhaps most importantly. I \emph{finally} have access to \textit{Neues Deutschland's} online archive. This means I am now able to see what kind of text I am able to save. The website offers text picked up by their own OCR technology, along with high resolution scans of the newspaper pages. Since the text they provide comes from OCR, there have been instances of errors that appear in the text. I am therefore more resolved to clean the data at the front end by visually confirming whether the text from the OCR matches the high resolution page scans. Whilst this process may seem tedious, I cannot envisage of a process that would automate this. It simply requires a human eye to confirm the data is correct.
    \item I also realised that I may need to create another user story. When the text is saved in a .txt file, it saves not only the title, but the subtitle, date and tagline information. This user story would then help with saving the articles in a format that would allocate a line for each type of information. My proposal would be saving the .txt files in this format
    \begin{verbatim}
        Date
        Article title
        Article subtitle
        Information on the author
        BLANK LINE
        Start of the body text
    \end{verbatim}
    This would help with textual analysis later on because I imagine I could write a script in Terminal that is able to copy selected .txt files and extract either the Body of the text, the article name, date, etc. This will mean that words found in the titles will not appear in the analysis of the body text. Or analysis of just titles will be possible.
    \item I still have not had any luck finding an API for Voyant. I feel like giving up on this front and focus instead on writing a script that will help me manipulate the .txt files.
\end{itemize}

\newpage
\subsection{Proof of Concept Design - User Stories}

The majority of user stories below were conceived of during the Elaboration Phase. New user stories have since been conceived of since I have gained access to the source material I will be dealing with. I have reintroduced the user story of \textbf{Save sources} from Elaboration I, whilst I have added the new user story \textbf{Extract the specific information I want from the .txt files}.

\subsubsection{Save sources}

As a research student, I would like to save the text from the newspaper articles into a .txt format that allows for an easier extraction of the information I want.

\subsubsection{Extract the specific information I want from the .txt files}

As a research student, I would like to be able to run a script that is able to target specific sections of the text for analysis, i.e. titles, dates, the body of the text, authors, for more concise analysis.

\subsubsection{Identify Themes}

As a research student, I would like to use software that is able to analyse a corpus of text and identify emergent themes thereof.

\subsubsection{Store Analysis}

As a research student, I would like to store the data captured from the textual analysis onto my machine.

\subsubsection{Additional Notes}

As a research student, I would like to store any notes I have written about the analysis separate to the data captured so not to corrupt the original data.

\subsubsection{Create Tags}

As a research student, I would like to create a set of tags that will help me organise all my data (including articles, textual analysis data and research notes.

\subsubsection{Search My Research}

As a research student, I would like software that would allow me to search through all newspaper articles, data from textual analysis and notes.

\subsubsection{Tag Newspaper Articles}

As a research student, I would like software that would allow me to tag themes or keywords to the newspaper articles that have been analysed.

\subsubsection{Tag Analysis}

As a research student, I would like software that would allow me to tag themes or keywords to the data captured from the textual analysis.

\subsubsection{Tag Notes}

As a research student, I would like software that would allow me to tag themes or keywords to the notes that I have written as part of my research.

\newpage
\subsection{Proof of Concept - User Stories Testing}

\subsubsection{Save sources}

\subsubsection{Extract the specific information I want from the .txt files}

The first part of this step needs to be manually performed. I need to select which articles (in their .txt file format) will be part of a corpus that will be analysed. I will then copy the original files into a directory called 'corpus\_input'. From here I am testing if I can make a script that will extract whatever information I want out of the corpus and put it into a new directory called 'corpus\_output'.

\textbf{17/10/19 - 10:30pm}

\textbf{Objective:} Read only the title of the article in a .txt file.

Because the title of the article is on line 2 of the .txt file, I should be able to use a code to get this line.

\textbf{Action:} Using the following code: \begin{verbatim}
    sed -n 2p *.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returns with the result 'Unverantwortlicher Entscheid', which is the title of the first article. I'm not sure why it only return one .txt's title when I used the wildcard *.txt.

\textbf{17/10/19 - 10:45pm}

Maybe I should try the head and tail commands as suggested in the Unix SWC lesson.

\textbf{Objective:} Read only the title of the article in a .txt file using the head and tail commands in a loop.

\textbf{Action:} Used the following code:\begin{verbatim}
    for filename in 1973-11-13_UnverantwortlicherEntscheid.txt
    1973-11-21_FIFA-Haltungbefremdend.txt 
    do
    head -n 2 $filename | tail -n 1
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned \begin{verbatim}
    Unverantwortlicher Entscheid
    FIFA-Haltung befremdend
\end{verbatim} which are the titles of the two articles!

\textbf{17/10/19 - 10:55pm}

Now to see if I can work with variables to get the titles from \emph{all} the .txt files.

\textbf{Objective:} Extract all the titles from every single .txt in the corpus\_input directory.

\textbf{Action:} Used the commands:\begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned this \begin{verbatim}
Unverantwortlicher Entscheid
Australien qualifiziert
Kurz berichtet
Die falschen Garantien der chilenischen Junta
Bulgarien qualifiziert
Holland nach 0:0 gegen Belgien qualifiziert
FIFA-Haltung befremdend
Nun im Nepstadion Sieg für Nationalelf 1:0
\end{verbatim} which is correct!

\textbf{17/10/19 - 11:09pm}

Now to see if I can create a new text file with all titles in it.

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    cat $filename >> articletitles.txt
    done
\end{verbatim}

\textbf{Error:} Yes, and error! It didn't work exactly as planned. 

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files. I think this is because the \begin{verbatim}
    cat $filename >> articletitles.txt
\end{verbatim} command wasn't piped the the earlier \begin{verbatim}
    do head -n 2 $filename | tail -n 1
\end{verbatim} command.

\textbf{17/10/19 - 11:16pm}

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 | cat $filename >> articletitles.txt
    done\end{verbatim}

\textbf{Error:} Same error as before...

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files.

\textbf{17/10/19 - 11:38pm}

After conceptualising it step by step in my mind, I think I know the code I should use.

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    done > articletitles.txt \end{verbatim}

\textbf{Error:} None! Finally!

\textbf{Result:} articletitles.txt file created with ALL the titles from the articles put on their own line.

\textbf{17/10/19 - 11:46pm}

The last step is to move the newly created articletitles.txt file to the corpus\_output directory ready for analysis.

\textbf{Move the newly created articletitles.txt file to the corpus\_output directory}

\textbf{Action:} Use the following command:\begin{verbatim}
    mv articletitles.txt ../corpus_output/articletitles.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} articletitles.txt moved from corpus\_input directory to the corpus\_output directory.

\textbf{18/10/19 - 12:25pm}

Now that I have figured out how to extract just the title, I want to see if I can write a command that will extract the title and the subtitle from a .txt file.

\textbf{Objective:} Extract the title and the subtitle from a .txt file.

\textbf{Action:} Use the code: \begin{verbatim}
    head -n 3 1973-11-13_UnverantwortlicherEntscheid.txt | tail -n 2
\end{verbatim} (because I know this article definitely has a subtitle)

\textbf{Error:} None.

\textbf{Result:} Terminal returned this \begin{verbatim}
    Unverantwortlicher Entscheid
    Generalsekretariat der FIFA brüskiert die Weltöffentlichkeit
\end{verbatim} which is the title and subtitle. So success!

\textbf{18/10/19 - 12:28pm}

Now to see if I can do it as a loop to target all the .txt files in the corpus\_input directory.

\textbf{Objective:} Extract the title and the subtitle from all .txt files in the corpus\_input directory.

\textbf{Action:} Used the code: \begin{verbatim}
    for filename in *.txt
    do head -n 3 $filename | tail -n 2
    done \end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned this \begin{verbatim}
    Unverantwortlicher Entscheid
    Generalsekretariat der FIFA brüskiert die Weltöffentlichkeit
    Australien qualifiziert

    Kurz berichtet

    Die falschen Garantien der chilenischen Junta

    Bulgarien qualifiziert

    Holland nach 0:0 gegen Belgien qualifiziert

    FIFA-Haltung befremdend
    TASS: Augen vor den bekannten Tatsachen verschlossen
    Nun im Nepstadion Sieg für Nationalelf 1:0
    Die Entscheidung fiel durch ein Kopfballtor von Lauck
\end{verbatim} which are the titles and subtitles from all the articles in the corpus\_input directory. Another success!

\textbf{18/10/19 - 12:34pm}

Last step for the titles and subtitles.

\textbf{Objective:} Create a new text file filled with all the titles of the articles and move that file to the corpus\_output directory.

\textbf{Action:} Use the code: \begin{verbatim}
    for filename in *.txt
    do head -n 3 $filename | tail -n 2
    done > articletitlessubtitles.txt
    mv articletitlessubtitles.txt ../corpus_output/articlestitlessubtitles.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success! A new file called articlestitlessubtitles.txt was created in the corpus\_output directory. This .txt file contains all the titles and subtitles from the newspaper articles.

\textbf{18/10/19 - 1:25pm}

I am currently at consultation hours to ask Brian some questions. After being initially happy with what I did last night regarding Terminal, I realised I need to create the same number of .txt files that I have from the input. This will help with better analysis in Voyant.

\textbf{Objective:} Extract titles from the .txt files in corpus\_input and create corresponding files in corpus\_output with just the titles.

\textbf{Action:} Used the code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 > ../corpus_output/title-${filename}
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success! New .txt files created in corpus\_output that contain just the titles.

\textbf{18/10/19 - 1:35pm}

Taking a step back, I now realised I may need to make mroe directories that are more specific with the output it contains. I may need to develop commands that creates these directories.

\textbf{Objective:} Write commands that create the directories necessary for this PoC.

\textbf{Action:} Use the code: \begin{verbatim}
    mkdir data
    mkdir data/corpus_input/
    mkdir data/corpus_output/
    mkdir data/corpus_output/titles/
    mkdir data/corpus_output/titlessubtitles/
    mkdir data/corpus_output/body/
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success! All directories were created. This step would probably be the first thing that needs to happen.

\textbf{18/10/19 - 1:45pm}

\textbf{Objective:} Create .txt files in the corpus\_output/titles/ directory that contain titles only.

\textbf{Action:} Back in the corpus\_input directory. Use the code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 > ../corpus_output/titles/title-${filename}
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success. .txt files created in the corpus\_output/titles/ directory.

\textbf{18/10/19 - 5:00pm}

\textbf{Objective:} Create .txt files in the corpus\_output/titlessubtitles/ directory that contain titles and subtitles.

\textbf{Action:} Use the code: \begin{verbatim}
    for filename in *.txt
    do head -n 3 $filename | tail -n 2 >
    ../corpus_output/titlessubtitles/titlesubtitles-${filename}
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Succcess. Objective achieved.

\subsubsection{Identify Themes}

\textbf{16/9/19 - 4:05pm}

\textbf{Objective:} See if Voyant Tools is able to analyse a corpus of text to see if there are any themes that emerge.

\textbf{Action:}
\begin{enumerate}
    \item Go to www.voyant-tools.org
    \item Select upload
    \item Navigate to the directory on my machine where the corpus is selected
    \item Select all the .txt files that I want analysed
    \item Select reveal
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Voyant Tools successfully analyses a corpus of text.

\textbf{10/10/19 - 11:40pm}

\textbf{Objective:} See if I can run Voyant Tools on my computer.

\textbf{Action:}

\begin{itemize}
    \item Downloaded Voyant Tools .zip file from Voyant Server website
    \item Unpackaged .zip file
    \item Run VoyantServer.jar
\end{itemize}

\textbf{Error:} VoyantServer.jar would not open

\textbf{Result:} VoyantServer.jar wouldn't run because it needs to install Java Development Kit to run.

\textbf{10/10/19 - 11:49pm}

\textbf{Objective:} See if I can run Voyant Tools on my computer after installing JDK.

\textbf{Action:}

\begin{itemize}
    \item Downloaded Voyant Tools .zip file from Voyant Server website
    \item Unpackaged .zip file
    \item Run VoyantServer.jar
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} VoyantServer is running. It also opens up a window in my default browser.

\includegraphics[width=\textwidth]{voyantserver.png}

\subsubsection{Store Analysis}

\textbf{16/9/19 - 4:24pm}

\textbf{Objective:} Find out what can be exported as data from Voyant Tools.

\textbf{Action:} Click on every 'Export' button to see what was applicable

\textbf{Error:} None.

\textbf{Result:} The following can be exported as .tsv files:
\begin{itemize}
    \item Terms
    \item Document Terms
    \item Documents
    \item Phrases
    \item Contexts
    \item Correlations
\end{itemize}

All other analysis outputs can be exported as Visualitions in the form of a .png file.

\textbf{16/9/19 - 4:37pm}

\textbf{Objective:} Store 'Terms' as a .tsv file
\begin{enumerate}
    \item Click on Export in the Terms window in Voyant Tools
    \item Click 'export all available data as tab separated values (text)'
    \item Copy the text from the new window that opens
    \item Open TextEdit on my machine
    \item Paste text into TextEdit
    \item Save as terms.tsv in the /Jugueta/MRes/data/textualanalysis/ directory
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} New filed saved as terms.tsv

\subsubsection{Additional Notes}

\textbf{16/9/19 - 4:52pm}

\textbf{Objective:} Store additional notes I have made on the analysis onto my machine

\textbf{Action:}
\begin{enumerate}
    \item Open TextEdit
    \item Write notes
    \item Save notes a .txt file
    \item Save in the /Jugueta/MRes/notes/textualanalysis/ directory on my machine
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Am able to use TextEdit to save notes on my machine.

\subsubsection{Create Tags}

\textbf{19/9/19 - 10:25am}

\textbf{Objective:} Create a 'stasi' tag that will be used to organise my data and notes relating to the East German Secret Police using Open Semantic Desktop Search.

\textbf{Action:}
\begin{enumerate}
    \item Launch VirtualBox VM
    \item Set the Shared Folder to /Jugueta/MRes/
    \item Start Open Semantic Desktop Search
    \item Click on the Activities menu in Open Semantic Desktop Search
    \item Click on Index documents to ensure that I am working with the right directory
    \item Click on Manage structure
    \item Click on Add new entry
    \item Enter 'stasi' for the name and select tag in Facet type
    \item Click Save
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} A 'stasi' tag is now in existence.

\subsubsection{Search My Research}

\textbf{19/9/19 - 10:59am}

\textbf{Objective:} Search for 'Hamburg' using Open Semantic Desktop Search

\textbf{Action:}
\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type in 'Hamburg' in the Search bar
    \item Click on Search
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} 38 results were returned with the query 'Hamburg'. This has searched my entire MRes folder though, so I want to see if there is a way to be more succinct with the search.

\textbf{19/9/19 - 11:37am}

\textbf{Objective:} Be more succinct with my searches, targeting a specific path or folder.

\textbf{Action:}
\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type in 'Hamburg' in the Search bar
    \item In Paths, select MRes
    \item Select notes
    \item Select articles
    \item Click on Search
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} Search was contained only in the articles folder. It returned 5 items.

\textbf{19/9/19 - 11:43am}

\textbf{Objective:} Figure out what is achievable in the search option settings.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Used numerous combinations of search phrases to test the search option settings.
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} Open Semantic Desktop Search has powerful bolean search operators. So it's just like MultiSearch. It also has fuzzy search options, that will return other word forms similar to the query (grammar and stemming).

\subsubsection{Tag Newspaper Articles}

\textbf{25/9/19 - 10:55am}

\textbf{Objective:} Tag 'stasi' to newspaper articles that are related to the Stasi.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type stasi in the search field
    \item Set the path to the newspaper directory
    \item Click on first returned result
    \item Confirm if relevant to Stasi
    \item Click the Tagging \& annotation button
    \item Go to the Tags tab
    \item Click on the stasi tag
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} The newspaper article is correctly tagged with the 'stasi' tag.

\subsubsection{Tag Textual Analysis}

\textbf{25/9/19 - 11:07am}

\textbf{Objective:} Tag 'stasi' to Voyant Tools Textual analysis data that are related to the Stasi.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type stasi in the search field
    \item Set the path to the textual analysis directory
    \item Click on first returned result
    \item Confirm if relevant to Stasi
    \item Click the Tagging \& annotation button
    \item Go to the Tags tab
    \item Click on the stasi tag
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} The data from the textual analysis is correctly tagged with the 'stasi' tag.

\subsubsection{Tag Notes}

\textbf{25/9/19 - 11:21am}

\textbf{Objective:} Tag 'stasi' to general research notes that I have written that are related to the Stasi.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type stasi in the search field
    \item Set the path to the notes directory
    \item Click on first returned result
    \item Confirm if relevant to Stasi
    \item Click the Tagging \& annotation button
    \item Go to the Tags tab
    \item Click on the stasi tag
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} The notes are correctly tagged with the 'stasi' tag.

\newpage
\subsection{Errors in Proof of Concept}

\textbf{10/10/19 - 11:40pm}

\textbf{Objective:} See if I can run Voyant Tools on my computer.

\textbf{Action:}

\begin{itemize}
    \item Downloaded Voyant Tools .zip file from Voyant Server website
    \item Unpackaged .zip file
    \item Run VoyantServer.jar
\end{itemize}

\textbf{Error:} VoyantServer.jar would not open

\textbf{Result:} VoyantServer.jar wouldn't run because it needs to install Java Development Kit to run.

\textbf{17/10/19 - 11:09pm}

Now to see if I can create a new text file with all titles in it.

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    cat $filename >> articletitles.txt
    done
\end{verbatim}

\textbf{Error:} Yes, and error! It didn't work exactly as planned. 

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files. I think this is because the \begin{verbatim}
    cat $filename >> articletitles.txt
\end{verbatim} command wasn't piped the the earlier \begin{verbatim}
    do head -n 2 $filename | tail -n 1
\end{verbatim} command.

\textbf{17/10/19 - 11:16pm}

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 | cat $filename >> articletitles.txt
    done\end{verbatim}

\textbf{Error:} Same error as before...

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files.

\newpage
\subsection{Proof of Concept Solutions}

\subsubsection{Make directories}

The following code will create the directories needed for this PoC. This would need to occur before any other script is run.

\begin{verbatim}
    mkdir data
    mkdir data/corpus_input/
    mkdir data/corpus_output/
    mkdir data/corpus_output/titles/
    mkdir data/corpus_output/titlessubtitles/
    mkdir data/corpus_output/body/
\end{verbatim}

\subsubsection{Extracting titles from the selection of articles}

Using the following code, one could extract just the title from each of the newspaper articles that are in .txt format within the corpus\_input directory and create an articletitles.txt file in the corpus\_output directory.

\begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    done > articletitles.txt
    mv articletitles.txt ../corpus_output/articletitles.txt
\end{verbatim}

The next step is to recreate these set of commands as a script.

\textbf{UPDATE}

The following code is able to create new .txt files in the corpus\_output directory that contain the title only.

\begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 > ../corpus_output/title-${filename}
    done
\end{verbatim}

\subsubsection{Extracting titles and subtitles from the selection of articles}

Using the following code, one could extract the titles and subtitles from each of the newspaper articles that are in .txt format within the corpus\_input directory and create an articletitles.txt file in the corpus\_output directory.

\begin{verbatim}
    for filename in *.txt
    do head -n 3 $filename | tail -n 2
    done > articletitlessubtitles.txt
    mv articletitlessubtitles.txt ../corpus_output/articlestitlessubtitles.txt
\end{verbatim}

\newpage
\section{Misc Notes on FOAR705}

\textbf{4/6/19 - 11:06am}

I have decided to keep using this Learning Journal for the rest of the unit. I will still upload them to Cloudstor on a weekly basis for consistency. I have chosen to do this so that I will have a more comprehensive journal that I can refer back to later in the unit.

Another general note. I still think my OSP is too vague. I don't know how to make it more specific. Hopefully I will get some guidance about it today in class. I really don't have time to go to the consultation hours due to the immense workload that I have from this unit, on top with all my other units.

\end{document}
