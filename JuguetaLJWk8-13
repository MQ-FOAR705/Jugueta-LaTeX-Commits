\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\title{FOAR705 - Learning Journal From Week 8 to 13}
\author{Jan Jugueta - 44828020}
\date{September, October \& November 2019}

\begin{document}

\maketitle

\tableofcontents

\newpage
\section{OpenRefine Lessons}

\subsection{Creating a new OpenRefine project}

\textbf{9/9/19 - 2:01pm}

\textbf{Objective:} Create a new project

\textbf{Action:}

\begin{enumerate}
    \item Clicked on create project
    \item Selected get data from 'This Computer'
    \item Clicked on Choose Files
    \item Selected SAFI\_openrefine.csv
    \item Clicked Next
    \item Clicked Create Project
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} New project created with OpenRefine using SAFI\_openrefine.csv.

\subsection{Using Facets}

\textbf{9/9/19 - 2:11pm}

\textbf{Objective:} Use faceting to look for potential errors in data entry in the village column.

\textbf{Action:}

\begin{enumerate}
    \item Selected Text Facet in the Facet in the drop down menu from the village column
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} The facet window on the left shows all the data entries for the village column. With this view we can see the errors that have been made. Chirdozo is likely a typo. Ruca is like a typo as well. There are many entries for Ruaca - Nhamuenda. 49 is almost certainly an error, but I have no idea what it refers to so there's not much I can do about that one.

\subsection{Using clustering to detect possible typing errors}

\textbf{9/9/19 - 2:35pm}

\textbf{Objective:} To merge clusters to clean up the values in the village column

\textbf{Action:}

\begin{enumerate}
    \item Click Cluster in the village Text Facet
    \item Select the key collision method and the metaphone3 keying function
    \item Click the Merge? box beside the clusters
    \item Click Merge Selected and Recluster
    \item Tried all other Methods and Keying Functions
    \item Go back to the village Text Facet
    \item Hovered over Chirdozo and selected edit
    \item Renamed Chirdozo to Chirodzo
    \item Hovered over Ruaca-Nhamuenda and selected edit
    \item Renamed it to Ruaca.
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} There are now only 4 different values in the village column. Chirodzo, God, Ruaca and 49. Another point was that the other methods and keying functions did not find any more clusters.

\subsection{Transforming data}

\textbf{9/9/19 - 2:46pm}

\textbf{Objective:} Remove the left square brackets from the data in the items\_owned column.

\textbf{Action:}

\begin{enumerate}
    \item Clicked on the arrow at the top of the items\_owned column
    \item Select Edit Cells
    \item Select Transform...
    \item Type in \begin{verbatim}
        value.replace("[", "")]
    \end{verbatim}
    \item Click OK
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} All left brackets have been removed.

\subsection{Trim Leading and Trailing Whitespace}

\textbf{9/9/19 - 3:12pm}

\textbf{Objective:} Tidy up respondent\_wall\_type so that burntbricks and muddaub are just one value instead of two.

\textbf{Action:}

\begin{enumerate}
    \item Selected Edit cells
    \item Selected Common transforms
    \item Selected Trim leading and trailing whitespace
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} There are only four choices left in the text facet.

\subsection{Numbers}

\textbf{22/9/19 - 8:55pm}

\textbf{Objective:} Convert years\_farm column to numbers.

\textbf{Action:}

\begin{enumerate}
    \item Click the drop down arrow in the years\_farm column
    \item Click Edit cells
    \item Click Common transforms...
    \item Click To number
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} years\_farm column data converted to numbers.

\subsection{Saving your work as a script}

\textbf{22/9/19 - 9:11pm}

\textbf{Objective:} Save script in JSON format as .txt file

\textbf{Action:}

\begin{enumerate}
    \item Click on Undo / Redo
    \item Click Extract
    \item Copy the code from the window on the right hand side
    \item Paste it TextEdit
    \item Save as Plain text file
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Script saved in JSON format as .txt file.

\subsection{Importing a script to use against another data set}

\textbf{22/9/19 - 9:14pm}

\textbf{Objective:} Import script to another data set

\textbf{Action:}

\begin{enumerate}
    \item Started a new OpenRefine project
    \item Click on Undo / Redo
    \item Click on Apply
    \item Paste the contents of the JSON .txt file
    \item Click on Perform operations
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Script applied to new project. Data set has been cleaned.

\subsection{Exporting}

\textbf{22/9/19 - 9:22pm}

\textbf{Objective:} Export OpenRefine project

\textbf{Action:}

\begin{enumerate}
    \item Click Export
    \item Click Export project
    \item Download tar.gz file
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Successfully exported OpenRefine project.

Answering the additional questions. There is a history folder with with change.txt files that contain records of each individual change in the data. There is also a data.zip file which contains all the data.

\subsection{Exporting Cleaned Data}

\textbf{22/9/19 - 9:27pm}

\textbf{Objective:} Export just the cleaned data

\textbf{Action:}

\begin{enumerate}
    \item Click on Export
    \item Select Comma-separated values
    \item Download .csv file
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Successfully exported cleaned data.

\subsection{Using online resources to get help with OpenRefine}

I visited the OpenRefine documentation wiki website. I found it has useful step-by-step guides on how to perform tasks. Like other wiki sites, it is relatively easily to follow. Finding the right areas was a breeze with how it has been categorised.

\newpage
\section{OpenRefine Exercises}

\subsection{Using Facets}

\textbf{9/9/19 - 2:17pm}

\begin{enumerate}
    \item There are 19 different interview\_date values
    \item It appears as it is formatted by Text.
    \item \textbf{Objective:} Change format to Date.
    
    \textbf{Action:}
    \begin{enumerate}
        \item Clicked the drop down menu in the interview\_date column
        \item Selected Edit cells
        \item Selected Common transforms
        \item Selected To date
    \end{enumerate}
    
    \textbf{Error:} None
    
    \textbf{Result:} Data format has been changed to date. 2016-11-16T00:00:00Z is an example of the format.
    \item Most of the data was collected in November 2016.
\end{enumerate}

\subsection{Transforming data}

\textbf{9/9/19 - 2:50pm}

\textbf{Objective:} Remove the right square brackets, single quote marks and spaces from the data in the items\_owned column.

\textbf{Action:}

\begin{enumerate}
    \item Clicked on the arrow at the top of the items\_owned column
    \item Select Edit Cells
    \item Select Transform...
    \item Type in \begin{verbatim}
        value.replace("'", "")
        value.replace("]", "")
        value.replace(" ", "")
    \end{verbatim}
    \item Click OK
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} All right brackets, single quote marks and spaces have been removed.

\textbf{9/9/19 - 2:54pm}

By sorting with count we can see which are the most commonly owned items. They are the mobile phone and radio. The least commonly owned items are cars and computers.

\textbf{9/9/19 - 2:57pm}

\textbf{Objective:} Finding which month(s) farmers were more likely to lack food

\textbf{Action:}

\begin{enumerate}
    \item Select Transform... in the Edit cells drop down menu in the months\_lack\_food column.
    \item Enter expression \begin{verbatim}
        value.replace("[", "").replace("]", "").replace(" ", "").replace("'", "")
    \end{verbatim}
    \item Create Custom text facet for months\_lack\_food column.
    \item Enter expression \begin{verbatim}
        value.split(";")
    \end{verbatim}
    \item Sort by count
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} November was the most common month that farmers lacked food.

\textbf{9/9/19 - 3:04pm}

\textbf{Objective:} Clean up months\_no\_water, liv\_owned, res\_change, and no\_food\_mitigation columns.

\textbf{Action:}

\begin{enumerate}
    \item Clicked on months\_no\_water column
    \item Selected Transform...
    \item Went to the History tab
    \item Reused the last expression in history
    \item Repeated the process for liv\_owned, res\_change, and no\_food\_mitigation columns
\end{enumerate}

\textbf{Error:} None

\textbf{Result:} Cleaned up months\_no\_water, liv\_owned, res\_change, and no\_food\_mitigation columns.

\subsection{Using undo and redo}

\textbf{9/9/19 - 3:09pm}

The undo and redo functions work just like explained in the lesson.

\subsection{Filtering}

\textbf{10/9/19 - 4:48pm}

\begin{enumerate}
    \item The roof types returned are mabatipitched and mabatisloping
    \item To restrict it further, you could be more specific with the spelling.
\end{enumerate}

\subsection{Excluding entries}

\textbf{10/9/19 - 5:05pm}

Have played with the include / exclude option. A reminder that this option appears in the facet, not the filter.

\subsection{Sort}

\textbf{10/9/19 - 5:10pm}

\textbf{Objective:} Find out if there are incorrect altitudes in the gps\_Altitude column.

\textbf{Action:}

\begin{enumerate}
    \item Go to the gps\_Altitude column
    \item Select sort in the triangle drop down menu
    \item Select Sort
    \item Select numbers
    \item Select smallest first
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} There are a few 0's. This is most likely missing data that was not recorded.

\subsection{Sorting by multiple columns}

\textbf{10/9/19 - 5:18pm}

When sorting by GPS coordinates, village 49 seems to match with Chirodzo. When using the interview date, it also seems to suggest Chirodzo.

\subsection{Numbers}

\textbf{22/9/19 - 9:02pm}

Only numerals can be transformed into numbers.

\subsection{Numeric facets}

\textbf{22/9/19 - 9:04pm}

The blank and non-numeric facets were ticked, presumably because of the 'abc' and blank data.

\newpage
\section{OpenRefine Lesson Notes}

\begin{itemize}
    \item Data is often very messy. OpenRefine can help organise messy data.
    \item OpenRefine records all changes to the data.
    \item OpenRefine does not modify the original dataset.
    \item OpenRefine is open source.
    \item Facets help get an overview of the data as well as bring consistency to the data.
\end{itemize}

\section{OpenRefine Error List}

No errors occured when completing the OpenRefine exercises and lessons.

\newpage
\section{R for Social Scientists Lessons}

\subsection{Create a new project}

\textbf{9/10/19 - 1:24pm}

\textbf{Objective:} Create a new project in R Studio

\textbf{Action:}

\begin{itemize}
    \item Open RStudio
    \item Click File - New project - New directory - New project
    \item Enter the name data-carpentry
\item Click Create project
\item Click File - New File - R script
\item Click save icon and name script "script.R"
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} New data-carpentry project created along with script.R

\subsection{Downloading the data and getting set up}

\textbf{9/10/19 - 1:34pm}

\textbf{Objective:} Create the directories "data", "data\_output" and "fig\_output"

\textbf{Action:} Type in the code
\begin{verbatim}
    dir.create("data")
    dir.create("data_output")
    dir.create("fig_output")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} The three directories were created.

\subsection{Presentation of the SAFI data}

\textbf{13/10/19 - 11:44am}

\textbf{Objective:} Load tidyverse and make an interviews object

\textbf{Action:} Used the following codes \begin{verbatim}
    library(tidyverse)
    interviews <- read_csv("data/SAFI_clean.csv", na = "NULL")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Tidyverse loaded and created the interviews object.

\subsection{Indexing and subsetting data frames}

\textbf{13/10/19 - 11:57am}

Just following with the lesson. I used the following codes in this section.

\begin{verbatim}
    ## first element in the first column of the data frame (as a vector)
    interviews[1, 1]
    ## first element in the 6th column (as a vector)
    interviews[1, 6]
    ## first column of the data frame (as a vector)
    interviews[[1]]
    ## first column of the data frame (as a data.frame)
    interviews[1]
    ## first three elements in the 7th column (as a vector)
    interviews[1:3, 7]
    ## the 3rd row of the data frame (as a data.frame)
    interviews[3, ]
    ## equivalent to head_interviews <- head(interviews)
    head_interviews <- interviews[1:6, ]
    interviews[, -1]          # The whole data frame, except the first column
    interviews[-c(7:131), ]   # Equivalent to head(interviews)
    interviews["village"]       # Result is a data frame
    interviews[, "village"]     # Result is a data frame
    interviews[["village"]]     # Result is a vector
    interviews$village          # Result is a vector
\end{verbatim}

\subsection{Factors}

\textbf{13/10/19 - 12:04pm}

I just followed along with the lesson and had no errors. Below is the list of code I inputted into RStudio:
\begin{verbatim}
    respondent_floor_type <- factor(c("earth", "cement", "cement", "earth"))
    levels(respondent_floor_type)
    nlevels(respondent_floor_type)
    respondent_floor_type # current order
    respondent_floor_type <- factor(respondent_floor_type, levels = c("earth", "cement"))
    respondent_floor_type # after re-ordering
    levels(respondent_floor_type)
    levels(respondent_floor_type)[2] <- "brick"
    levels(respondent_floor_type)
    respondent_floor_type
\end{verbatim}

\subsection{Converting factors}

\textbf{13/10/19 - 12:09pm}

\begin{verbatim}
    as.character(respondent_floor_type)
    year_fct <- factor(c(1990, 1983, 1977, 1998, 1990))
    as.numeric(year_fct)                     # Wrong! And there is no warning...
    as.numeric(as.character(year_fct))       # Works...
    as.numeric(levels(year_fct))[year_fct]   # The recommended way.
\end{verbatim}

\subsection{Renaming factors}

\textbf{13/10/19 - 12:15pm}

SO I have kept on following the lesson. I used these codes: \begin{verbatim}
    ## create a vector from the data frame column "memb_assoc"
    memb_assoc <- interviews$memb_assoc
    ## convert it into a factor
    memb_assoc <- as.factor(memb_assoc)
    ## let's see what it looks like
    memb_assoc
    ## bar plot of the number of interview respondents who were
    ## members of irrigation association:
    plot(memb_assoc)
\end{verbatim}

I noticed that a plot appeared in one of the panes. I also noticed I could export it, so I decided to do that. I set the export path to point to fig\_output directory. This is what the image looks like.

\includegraphics[width=\textwidth]{Rplot.png}

Continuing with the lesson, I entered these codes: \begin{verbatim}
    memb_assoc <- interviews$memb_assoc
    memb_assoc[is.na(memb_assoc)] <- "undetermined"
    memb_assoc <- as.factor(memb_assoc)
    memb_assoc
    plot(memb_assoc)
\end{verbatim}

And got this plot.

\includegraphics[width=\textwidth]{Rplot01.png}

\subsection{Formatting dates}

\textbf{13/10/19 - 12:33pm}

Following on with the lesson. Experienced no errors when inputting these codes: \begin{verbatim}
    str(interviews)
    library(lubridate)
    dates <- interviews$interview_date
    str(dates)
    interviews$day <- day(dates)
    interviews$month <- month(dates)
    interviews$year <- year(dates)
    interviews
\end{verbatim}

\subsection{Learning dplyr and tidyr}

\textbf{22/10/19 - 5:49pm}

Reloaded tidyverse and reviewed the data.

\subsection{Selecting columns and filtering rows}

\textbf{22/10/19 - 5:52pm}

Inputed \begin{verbatim}
    select(interviews, village, no_membrs, years_liv)
\end{verbatim}

Returned \begin{verbatim}
    # A tibble: 131 x 3
   village  no_membrs years_liv
   <chr>        <dbl>     <dbl>
 1 God              3         4
 2 God              7         9
 3 God             10        15
 4 God              7         6
 5 God              7        40
 6 God              3         3
 7 God              6        38
 8 Chirodzo        12        70
 9 Chirodzo         8         6
10 Chirodzo        12        23
# … with 121 more rows
\end{verbatim}

Filtered for "God" with \begin{verbatim}
    filter(interviews, village == "God")
\end{verbatim}

Returned \begin{verbatim}
    # A tibble: 43 x 17
   key_ID village interview_date      no_membrs years_liv respondent_wall… rooms
    <dbl> <chr>   <dttm>                  <dbl>     <dbl> <chr>            <dbl>
 1      1 God     2016-11-17 00:00:00         3         4 muddaub              1
 2      1 God     2016-11-17 00:00:00         7         9 muddaub              1
 3      3 God     2016-11-17 00:00:00        10        15 burntbricks          1
 4      4 God     2016-11-17 00:00:00         7         6 burntbricks          1
 5      5 God     2016-11-17 00:00:00         7        40 burntbricks          1
 6      6 God     2016-11-17 00:00:00         3         3 muddaub              1
 7      7 God     2016-11-17 00:00:00         6        38 muddaub              1
 8     11 God     2016-11-21 00:00:00         6        20 sunbricks            1
 9     12 God     2016-11-21 00:00:00         7        20 burntbricks          3
10     13 God     2016-11-21 00:00:00         6         8 burntbricks          1
# … with 33 more rows, and 10 more variables: memb_assoc <chr>,
#   affect_conflicts <chr>, liv_count <dbl>, items_owned <chr>, no_meals <dbl>,
#   months_lack_food <chr>, instanceID <chr>, day <int>, month <dbl>, year <dbl>
\end{verbatim}

Not sure why my came back with 10 more variables as opposed to 8.

\subsection{Pipes}

\textbf{22/10/19 - 6:04pm}

Tried my hand at pipes in R using this \begin{verbatim}
    interviews %>%
    filter(village == "God") %>%
    select(no_membrs, years_liv)
\end{verbatim}

And it came back with this \begin{verbatim}
    # A tibble: 43 x 2
   no_membrs years_liv
       <dbl>     <dbl>
 1         3         4
 2         7         9
 3        10        15
 4         7         6
 5         7        40
 6         3         3
 7         6        38
 8         6        20
 9         7        20
10         6         8
# … with 33 more rows
\end{verbatim}

which was the same as the lesson. So good.

\textbf{22/10/19 - 6:20pm}

Keeping on with pipes. I put this in \begin{verbatim}
    interviews_god <- interviews %>%
    filter(village == "God") %>%
    select(no_membrs, years_liv

    interviews_god
\end{verbatim}

and got this out \begin{verbatim}
    # A tibble: 43 x 2
   no_membrs years_liv
       <dbl>     <dbl>
 1         3         4
 2         7         9
 3        10        15
 4         7         6
 5         7        40
 6         3         3
 7         6        38
 8         6        20
 9         7        20
10         6         8
# … with 33 more rows
\end{verbatim}

\subsection{Mutate}

\textbf{22/10/19 - 6:29pm}

Used this code \begin{verbatim}
    interviews %>%
    mutate(people_per_room = no_membrs / rooms)
\end{verbatim}

and had this returned, just like in the lesson. \begin{verbatim}
    # A tibble: 131 x 18
   key_ID village interview_date      no_membrs years_liv respondent_wall… rooms
    <dbl> <chr>   <dttm>                  <dbl>     <dbl> <chr>            <dbl>
 1      1 God     2016-11-17 00:00:00         3         4 muddaub              1
 2      1 God     2016-11-17 00:00:00         7         9 muddaub              1
 3      3 God     2016-11-17 00:00:00        10        15 burntbricks          1
 4      4 God     2016-11-17 00:00:00         7         6 burntbricks          1
 5      5 God     2016-11-17 00:00:00         7        40 burntbricks          1
 6      6 God     2016-11-17 00:00:00         3         3 muddaub              1
 7      7 God     2016-11-17 00:00:00         6        38 muddaub              1
 8      8 Chirod… 2016-11-16 00:00:00        12        70 burntbricks          3
 9      9 Chirod… 2016-11-16 00:00:00         8         6 burntbricks          1
10     10 Chirod… 2016-12-16 00:00:00        12        23 burntbricks          5
# … with 121 more rows, and 11 more variables: memb_assoc <chr>,
#   affect_conflicts <chr>, liv_count <dbl>, items_owned <chr>, no_meals <dbl>,
#   months_lack_food <chr>, instanceID <chr>, day <int>, month <dbl>, year <dbl>,
#   people_per_room <dbl>
\end{verbatim}

\textbf{22/10/19 - 6:32pm}

Now using a filter to remove NULL data

\begin{verbatim}
    interviews %>%
    filter(!is.na(memb_assoc)) %>%
    mutate(people_per_room = no_membrs / rooms)
\end{verbatim}

which returned \begin{verbatim}
    # A tibble: 92 x 18
   key_ID village interview_date      no_membrs years_liv respondent_wall… rooms
    <dbl> <chr>   <dttm>                  <dbl>     <dbl> <chr>            <dbl>
 1      1 God     2016-11-17 00:00:00         7         9 muddaub              1
 2      7 God     2016-11-17 00:00:00         6        38 muddaub              1
 3      8 Chirod… 2016-11-16 00:00:00        12        70 burntbricks          3
 4      9 Chirod… 2016-11-16 00:00:00         8         6 burntbricks          1
 5     10 Chirod… 2016-12-16 00:00:00        12        23 burntbricks          5
 6     12 God     2016-11-21 00:00:00         7        20 burntbricks          3
 7     13 God     2016-11-21 00:00:00         6         8 burntbricks          1
 8     15 God     2016-11-21 00:00:00         5        30 sunbricks            2
 9     21 God     2016-11-21 00:00:00         8        20 burntbricks          1
10     24 Ruaca   2016-11-21 00:00:00         6         4 burntbricks          2
# … with 82 more rows, and 11 more variables: memb_assoc <chr>,
#   affect_conflicts <chr>, liv_count <dbl>, items_owned <chr>, no_meals <dbl>,
#   months_lack_food <chr>, instanceID <chr>, day <int>, month <dbl>, year <dbl>,
#   people_per_room <dbl>
\end{verbatim}

! negates the result, so we are asking when it is NOT NULL.

\subsection{Split-apply-combine data analysis and the summarize() function}

\subsubsection{The summarize() function}

\textbf{22/10/19 - 7:01pm}

Using group by and summarize functions.

\begin{verbatim}
    interviews %>%
    group_by(village) %>%
    summarize(mean_no_membrs = mean(no_membrs))
\end{verbatim}

R returned \begin{verbatim}
    # A tibble: 3 x 2
  village  mean_no_membrs
  <chr>             <dbl>
1 Chirodzo           7.08
2 God                6.86
3 Ruaca              7.57
\end{verbatim}

\textbf{22/10/19 - 7:59pm}

Grouping by multiple columns.

Used this code \begin{verbatim}
    interviews %>%
    group_by(village, memb_assoc) %>%
    summarize(mean_no_membrs = mean(no_membrs))
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 9 x 3
# Groups:   village [3]
  village  memb_assoc mean_no_membrs
  <chr>    <chr>               <dbl>
1 Chirodzo no                   8.06
2 Chirodzo yes                  7.82
3 Chirodzo NA                   5.08
4 God      no                   7.13
5 God      yes                  8   
6 God      NA                   6   
7 Ruaca    no                   7.18
8 Ruaca    yes                  9.5 
9 Ruaca    NA                   6.22
\end{verbatim}

\textbf{22/10/19 - 8:05pm}

Excluding data from our table using a filter step. Use the code \begin{verbatim}
    interviews %>%
    filter(!is.na(memb_assoc)) %>%
    group_by(village, memb_assoc) %>%
    summarize(mean_no_membrs = mean(no_membrs))
\end{verbatim}

Returned this \begin{verbatim}
    # A tibble: 6 x 3
# Groups:   village [3]
  village  memb_assoc mean_no_membrs
  <chr>    <chr>               <dbl>
1 Chirodzo no                   8.06
2 Chirodzo yes                  7.82
3 God      no                   7.13
4 God      yes                  8   
5 Ruaca    no                   7.18
6 Ruaca    yes                  9.5 
\end{verbatim}

\textbf{22/10/19 - 8:08pm}

Summarise multiple variables at the same time. Used the code \begin{verbatim}
    interviews %>%
    filter(!is.na(memb_assoc)) %>%
    group_by(village, memb_assoc) %>%
    summarize(mean_no_membrs = mean(no_membrs),
              min_membrs = min(no_membrs))
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 6 x 4
# Groups:   village [3]
  village  memb_assoc mean_no_membrs min_members
  <chr>    <chr>               <dbl>       <dbl>
1 Chirodzo no                   8.06           4
2 Chirodzo yes                  7.82           2
3 God      no                   7.13           3
4 God      yes                  8              5
5 Ruaca    no                   7.18           2
6 Ruaca    yes                  9.5            5
\end{verbatim}

\textbf{22/10/19 - 8:12pm}

Rearrange the result of a query to inspect the values. Used the code \begin{verbatim}
    interviews %>% 
  filter(!is.na(memb_assoc)) %>% 
  group_by(village, memb_assoc) %>% 
  summarize(mean_no_membrs = mean(no_membrs), min_membrs = min(no_membrs)) %>% 
  arrange(min_membrs)
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 6 x 4
# Groups:   village [3]
  village  memb_assoc mean_no_membrs min_membrs
  <chr>    <chr>               <dbl>      <dbl>
1 Chirodzo yes                  7.82          2
2 Ruaca    no                   7.18          2
3 God      no                   7.13          3
4 Chirodzo no                   8.06          4
5 God      yes                  8             5
6 Ruaca    yes                  9.5           5
\end{verbatim}

\textbf{22/10/19 - 8:14pm}

Sort in descending order. Used this code \begin{verbatim}
    interviews %>% 
  filter(!is.na(memb_assoc)) %>% 
  group_by(village, memb_assoc) %>% 
  summarize(mean_no_membrs = mean(no_membrs), min_membrs = min(no_membrs)) %>%
  arrange(desc(min_membrs))
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 6 x 4
# Groups:   village [3]
  village  memb_assoc mean_no_membrs min_membrs
  <chr>    <chr>               <dbl>      <dbl>
1 God      yes                  8             5
2 Ruaca    yes                  9.5           5
3 Chirodzo no                   8.06          4
4 God      no                   7.13          3
5 Chirodzo yes                  7.82          2
6 Ruaca    no                   7.18          2
\end{verbatim}

\subsubsection{Counting}

\textbf{22/10/19 - 8:17pm}

Count the number of rows of data for each village. Used the code \begin{verbatim}
    interviews %>% 
  count(village)
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 3 x 2
  village      n
  <chr>    <int>
1 Chirodzo    39
2 God         43
3 Ruaca       49
\end{verbatim}

\textbf{22/10/19 - 8:18pm}

Using 'sort' to get the results in decreasing order. Used the code \begin{verbatim}
    interviews %>% 
  count(village, sort = TRUE)
\end{verbatim}

Returned with this \begin{verbatim}
    # A tibble: 3 x 2
  village      n
  <chr>    <int>
1 Ruaca       49
2 God         43
3 Chirodzo    39
\end{verbatim}

\subsection{Reshaping with gather and spread}

\subsubsection{Spreading}

\textbf{22/10/19 - 8:31pm}

Used the code \begin{verbatim}
    interviews_spread <- interviews %>%
    mutate(wall_type_logical = TRUE) %>%
    spread(key = respondent_wall_type, value = wall_type_logical, fill = FALSE)
\end{verbatim}

Not sure what happened here, if anything?

\subsection{Gathering}

\textbf{22/10/19 - 8:34pm}

Used the code \begin{verbatim}
    interviews_gather <- interviews_spread %>%
    gather(key = respondent_wall_type, value = "wall_type_logical",
           burntbricks:sunbricks)
\end{verbatim}

Also used the code \begin{verbatim}
    interviews_gather <- interviews_spread %>%
    gather(key = "respondent_wall_type", value = "wall_type_logical",
           burntbricks:sunbricks) %>%
    filter(wall_type_logical) %>%
    select(-wall_type_logical)
\end{verbatim}

Again, not sure what happened here...

\subsection{Applying spread() to clean our data}

\textbf{22/10/19 - 8:40pm}

Used the code \begin{verbatim}
    interviews_items_owned <- interviews %>%
  separate_rows(items_owned, sep=";") %>%
  mutate(items_owned_logical = TRUE) %>%
  spread(key = items_owned, value = items_owned_logical, fill = FALSE)

nrow(interviews_items_owned)
\end{verbatim}

Returned with \begin{verbatim}
    [1] 131
\end{verbatim}

\textbf{22/10/19 - 8:41pm}

Entered \begin{verbatim}
    interviews_items_owned <- interviews_items_owned %>%
    rename(no_listed_items = `<NA>`)
\end{verbatim}

\textbf{22/10/19 - 8:43pm}

Show the number of respondents in each village who owned a particular item \begin{verbatim}
    interviews_items_owned %>%
  filter(bicycle) %>%
  group_by(village) %>%
  count(bicycle)
\end{verbatim}

and returned \begin{verbatim}
    # A tibble: 3 x 3
# Groups:   village [3]
  village  bicycle     n
  <chr>    <lgl>   <int>
1 Chirodzo TRUE       17
2 God      TRUE       23
3 Ruaca    TRUE       20
\end{verbatim}

\textbf{22/10/19 - 8:45pm}

Calculate the average number of items from the list owned by respondents in each village. Used the code \begin{verbatim}
    interviews_items_owned %>%
    mutate(number_items = rowSums(select(., bicycle:television))) %>%
    group_by(village) %>%
    summarize(mean_items = mean(number_items))
\end{verbatim}

which returned \begin{verbatim}
    # A tibble: 3 x 2
  village  mean_items
  <chr>         <dbl>
1 Chirodzo       4.54
2 God            3.98
3 Ruaca          5.57
\end{verbatim}

\subsection{Exporting data}

\textbf{22/10/19 - 8:47pm}

Followed the lesson and used this code \begin{verbatim}
    interviews_plotting <- interviews %>%
    ## spread data by items_owned
    separate_rows(items_owned, sep=";") %>%
    mutate(items_owned_logical = TRUE) %>%
    spread(key = items_owned, value = items_owned_logical, fill = FALSE) %>%
    rename(no_listed_items = `<NA>`) %>%
    ## spread data by months_lack_food
    separate_rows(months_lack_food, sep=";") %>%
    mutate(months_lack_food_logical = TRUE) %>%
    spread(key = months_lack_food, value = months_lack_food_logical, fill = FALSE) %>%
    ## add some summary columns
    mutate(number_months_lack_food = rowSums(select(., Apr:Sept))) %>%
    mutate(number_items = rowSums(select(., bicycle:television)))
\end{verbatim}

and

\begin{verbatim}
    write_csv(interviews_plotting, path = "data_output/interviews_plotting.csv")
\end{verbatim}

\subsection{Data Visualisation with ggplot2}

\textbf{29/10/19 - 12:57pm}

Ran the following code in R 

\begin{verbatim}
    library(tidyverse)
interviews_plotting <- read_csv("data_output/interviews_plotting.csv")
\end{verbatim}

R returned this \begin{verbatim}
Parsed with column specification:
cols(
  .default = col_logical(),
  key_ID = col_double(),
  village = col_character(),
  interview_date = col_datetime(format = ""),
  no_membrs = col_double(),
  years_liv = col_double(),
  respondent_wall_type = col_character(),
  rooms = col_double(),
  memb_assoc = col_character(),
  affect_conflicts = col_character(),
  liv_count = col_double(),
  no_meals = col_double(),
  instanceID = col_character(),
  day = col_double(),
  month = col_double(),
  year = col_double(),
  number_months_lack_food = col_double(),
  number_items = col_double()
)
See spec(...) for full column specifications.
\end{verbatim}

\subsubsection{Plotting with ggplot2}

\textbf{29/10/19 - 12:59pm}

Used the code \begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = no_membrs, y = number_items)) +
    geom_point()
\end{verbatim}

R made this plot

\includegraphics[width=\textwidth]{ggplot_1.png}

\subsection{Building your plots iteratively}

\textbf{29/10/19 - 1:17pm}

The jitter function in plotting

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = no_membrs, y = number_items)) +
    geom_jitter(alpha = 0.5)
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_2.png}

Adding colour

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = no_membrs, y = number_items)) +
    geom_jitter(alpha = 0.5, color = "blue")
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_3.png}

Using different colours for different values in the vector.

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = no_membrs, y = number_items)) +
    geom_jitter(aes(color = village), alpha = 0.5)
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_4.png}

\subsection{Boxplot}

\textbf{29/10/19 - 1:27pm}

Using a boxplot.

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type, y = rooms)) +
    geom_boxplot()
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_6.png}

Adding points to a boxplot.

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type, y = rooms)) +
    geom_boxplot(alpha = 0) +
    geom_jitter(alpha = 0.5, color = "tomato")
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_7.png}

\subsection{Barplots}

\textbf{29/10/19 - 1:42pm}

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type)) +
    geom_bar()
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_11.png}

Using the fill aesthetic

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type)) +
    geom_bar(aes(fill = village))
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_12.png}

Separate the portions of the stacked bar.

\begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type)) +
    geom_bar(aes(fill = village), position = "dodge")
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_13.png}

Displaying the proportion of each housing type in each village.

\begin{verbatim}
    percent_wall_type <- interviews_plotting %>%
    filter(respondent_wall_type != "cement") %>%
    count(village, respondent_wall_type) %>%
    group_by(village) %>%
    mutate(percent = n / sum(n)) %>%
    ungroup()
\end{verbatim}

Plot the new information

\begin{verbatim}
     ggplot(percent_wall_type, aes(x = village, y = percent, fill = respondent_wall_type)) +
     geom_bar(stat = "identity", position = "dodge")
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_14.png}

\subsection{Adding Labels and Titles}

\textbf{29/10/19 - 2:02pm}

\begin{verbatim}
    ggplot(percent_wall_type, aes(x = village, y = percent, fill = respondent_wall_type)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title="Proportion of wall type by village",
         x="Wall Type",
         y="Percent")
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_16.png}

\subsection{Faceting}

\textbf{29/10/19 - 2:03pm}

Create multiple plots with faceting.

\begin{verbatim}
    ggplot(percent_wall_type, aes(x = respondent_wall_type, y = percent)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title="Proportion of wall type by village",
         x="Wall Type",
         y="Percent") +
    facet_wrap(~ village)
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_17.png}

\textbf{29/10/19 - 2:05pm}

White backgrounds with plots.

\begin{verbatim}
    ggplot(percent_wall_type, aes(x = respondent_wall_type, y = percent)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title="Proportion of wall type by village",
         x="Wall Type",
         y="Percent") +
    facet_wrap(~ village) +
    theme_bw() +
    theme(panel.grid = element_blank())
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_18.png}

View the proportion of respondents in each village who owned a particular item. First used.

\begin{verbatim}
    percent_items <- interviews_plotting %>%
    gather(items, items_owned_logical, bicycle:no_listed_items) %>%
    filter(items_owned_logical) %>%
    count(items, village) %>%
    ## add a column with the number of people in each village
    mutate(people_in_village = case_when(village == "Chirodzo" ~ 39,
                                         village == "God" ~ 43,
                                         village == "Ruaca" ~ 49)) %>%
    mutate(percent = n / people_in_village)
\end{verbatim}

then used this to plot.

\begin{verbatim}
    ggplot(percent_items, aes(x = village, y = percent)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ items) +
    theme_bw() +
    theme(panel.grid = element_blank())
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_19.png}

\subsection{Customization}

\textbf{29/10/19 - 2:11pm}

Changing the names of the axes to something more informative.

\begin{verbatim}
    ggplot(percent_items, aes(x = village, y = percent)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ items) +
    labs(title = "Percent of respondents in each village who owned each item",
         x = "Village",
         y = "Percent of Respondents") +
    theme_bw()
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_21.png}

Changing the font size to make it more readable.

\begin{verbatim}
    ggplot(percent_items, aes(x = village, y = percent)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ items) +
  labs(title = "Percent of respondents in each village who owned each item",
       x = "Village",
       y = "Percent of Respondents") +
  theme_bw() +
  theme(text=element_text(size = 8))
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_22.png}

\textbf{29/10/19 - 2:20pm}

Diagonally place font

\begin{verbatim}
    ggplot(percent_items, aes(x = village, y = percent)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ items) +
  labs(title = "Percent of respondents in each village \n who owned each item",
       x = "Village",
       y = "Percent of Respondents") +
  theme_bw() +
  theme(axis.text.x = element_text(colour = "grey20", size = 8, angle = 45, hjust = 0.5, vjust = 0.5),
        axis.text.y = element_text(colour = "grey20", size = 8),
        text = element_text(size = 8))
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_23.png}

\textbf{29/10/19 - 2:23pm}

Saving themes as objects and centering titles. First used

\begin{verbatim}
    grey_theme <- theme(axis.text.x = element_text(colour = "grey20", size = 8, angle = 45, hjust = 0.5, vjust = 0.5),
                    axis.text.y = element_text(colour = "grey20", size = 8),
                    text = element_text(size = 8),
                    plot.title = element_text(hjust = 0.5))
\end{verbatim}

then used \begin{verbatim}
    ggplot(percent_items, aes(x = village, y = percent)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ items) +
  labs(title = "Percent of respondents in each village \n who owned each item",
       x = "Village",
       y = "Percent of Respondents") +
  grey_theme
\end{verbatim}

\includegraphics[width=\textwidth]{ggplot_24.png}

\newpage
\section{R for Social Scientists Exercises}

\subsection{Installing additional packages using the packages tab}

\textbf{9/10/19 - 1:54pm}

\textbf{Objective:} Download tidyverse package

\textbf{Action:}
\begin{itemize}
    \item Click on Packages tab
\item Click on Install
\item Type in tidyverse
\item Click Install
\end{itemize}

\textbf{Error: None}

\textbf{Result:} Tidyverse installed, along with what looks like many other packages.

\subsection{Creating ojbects in R}

\textbf{9/10/19 - 4:36pm}

The value is still 6.175

\subsection{Comments}

\textbf{9/10/19 - 4:49pm}

\textbf{Objective:} See what happens when I create objects for width and length and get them to work with area.

\textbf{Action:} Entered the following code in:
\begin{verbatim}
    length <- 2
    width <- 1
    area <- length*witdh
\end{verbatim}

Then changed the value for length to 7.0 and width to 6.5. Retyped area.

\textbf{Error:} None.

\textbf{Result:} Area did not change after the values of length and width were changed. This is presumably because \begin{verbatim}
    area <- length*width
\end{verbatim} was not run again.

\subsection{Functions and their arguments}

\textbf{9/10/19 - 5:20pm}

Using \begin{verbatim}
    ?round
\end{verbatim} it seems that there are other functions similar to it. These include \begin{verbatim}
    ceiling(x)
    floor(x)
    trunc(x,...)
    signif(x,digits=6)
\end{verbatim}

\subsection{Vectors and data types}

\textbf{9/10/19 - 5:46pm}

If we try to mix different vectors together, R will convert them all to be the same type.

For the second part of this exercise, it seems that R will find the common denominator and convert them all to that same type.

For the third part of the exercise, only one value of TRUE is present, which is part of the \begin{verbatim}
    char_logical
\end{verbatim} object.

\subsection{Missing data}

\textbf{9/10/19 - 6:15pm}

\textbf{Objective:} Create a new vector with NAs removed then use \begin{verbatim}
    median()
\end{verbatim} to find the median of the rooms.

\textbf{Action:} Used the following codes:
\begin{verbatim}
    rooms_no_na <- na.omit(rooms)
    median(rooms,na.rm=TRUE)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} The median returned the value of 1.

\textbf{9/10/19 - 6:18pm}

\textbf{Objective:} Use R to figure out how many households in the set use more than 2 rooms for sleeping

\textbf{Action:} Used the following codes \begin{verbatim}
    rooms_above_2 <- rooms_no_na[rooms_no_na > 2]
length(rooms_above_2)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Returned the value of 4.

\subsection{Indexing and subsetting data frames}

\textbf{13/10/19 - 12:01pm}

For this exercise, I used these codes in this order:

\begin{verbatim}
    interviews_100 <- interviews[100, ]
    n_rows <- nrow(interviews)
    interviews_last <- interviews[n_rows, ]
    interviews_middle <- interviews[(n_rows / 2), ]
    interviews_head <- interviews[-(7:n_rows), ]
\end{verbatim}

\subsection{Renaming factors}

\textbf{13/10/19 - 12:24pm}

\textbf{Objective:} Rename the levels of the factor to have the first letter in uppercase: "No", "Undetermined", and "Yes".

\textbf{Action:} I used the following code: \begin{verbatim}
    levels(memb_assoc) <- c("No", "Undetermined", "Yes")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} They are now in uppercase.

\textbf{13/10/19 - 12:26pm}

\textbf{Objective:} Recreate the barplot so that "Undetermined" is last.

\textbf{Action:} I used the following code: \begin{verbatim}
    memb_assoc <- factor(memb_assoc, levels = c("No", "Yes", "Undetermined"))
    plot(memb_assoc)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Please see the below plot.

\includegraphics[width=\textwidth]{Rplot02.png}

\subsection{Pipes}

\textbf{22/10/19 - 6:25pm}

\textbf{Objective:} Using pipes, subset the interviews data to include interviews where respondents were members of an irrigation association (memb\_assoc) and retain only the columns affect\_conflicts, liv\_count, and no\_meals.

\textbf{Action:} Used this code \begin{verbatim}
    interviews %>%
    filter(memb_assoc == "yes") %>%
    select(affect_conflicts, liv_count, no_meals)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} RStudio returned me this. \begin{verbatim}
    # A tibble: 33 x 3
   affect_conflicts liv_count no_meals
   <chr>                <dbl>    <dbl>
 1 once                     3        2
 2 never                    2        2
 3 never                    2        3
 4 once                     3        2
 5 frequently               1        3
 6 more_once                5        2
 7 more_once                3        2
 8 more_once                2        3
 9 once                     3        3
10 never                    3        3
# … with 23 more rows
\end{verbatim}

\subsection{Mutate}

\textbf{22/10/19 - 6:44pm}

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared.

\textbf{22/10/19 - 6:50pm}

Trying again.

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code and entered each line individually this time. \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared. I don't get it. I'm moving on.

EDIT: It did work. I typed in \begin{verbatim}
    interviews_total_meals
\end{verbatim} and it had shown that contents of the object. Therefore it was user error.

\subsection{Split-apply-combine data analysis and the summarize() function - counting}

\textbf{22/10/19 - 8:21pm}

\textbf{Objective:} Find out how many households in the survey have an average of two meals per day? Three meals per day?

\textbf{Action:} Used the code \begin{verbatim}
    interviews %>% 
  count(no_meals)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} R returned this \begin{verbatim}
    # A tibble: 2 x 2
  no_meals     n
     <dbl> <int>
1        2    52
2        3    79
\end{verbatim}

Households who have two meals a day are 52 in number. 79 have three a day.

\textbf{22/10/19 - 8:24pm}

\textbf{Objective:} Use group\_by() and summarize() to find the mean, min, and max number of household members for each village.

\textbf{Action:} Used the code \begin{verbatim}
    interviews %>%
  group_by(village) %>%
  summarize(
      mean_no_membrs = mean(no_membrs),
      min_no_membrs = min(no_membrs),
      max_no_membrs = max(no_membrs),
      n = n()
  )
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} R returned this \begin{verbatim}
    # A tibble: 3 x 5
  village  mean_no_membrs min_no_membrs max_no_membrs     n
  <chr>             <dbl>         <dbl>         <dbl> <int>
1 Chirodzo           7.08             2            12    39
2 God                6.86             3            15    43
3 Ruaca              7.57             2            19    49
\end{verbatim}

\textbf{22/10/19 - 8:26pm}

\textbf{Objective:} Find out what was the largest household interviewed in each month?

\textbf{Action:} Loaded lubridate by using the code \begin{verbatim}
    library(lubridate)
\end{verbatim}

Then used this code \begin{verbatim}
    interviews %>%
  mutate(month = month(interview_date),
         day = day(interview_date),
         year = year(interview_date)) %>%
  group_by(year, month) %>%
  summarize(max_no_membrs = max(no_membrs))
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} R returned \begin{verbatim}
    # A tibble: 5 x 3
# Groups:   year [2]
   year month max_no_membrs
  <dbl> <dbl>         <dbl>
1  2016    11            19
2  2016    12            12
3  2017     4            17
4  2017     5            15
5  2017     6            15
\end{verbatim}

\subsection{Applying spread() to clean our data}

\textbf{22/10/19 - 8:49pm}

\textbf{Objective:} Create a new data frame (named interviews\_months\_lack\_food) that has one column for each month and records TRUE or FALSE for whether each interview respondent was lacking food in that month.

\textbf{Action:} Used the code \begin{verbatim}
    interviews_months_lack_food <- interviews %>%
  separate_rows(months_lack_food, sep=";") %>%
  mutate(months_lack_food_logical  = TRUE) %>%
  spread(key = months_lack_food, value = months_lack_food_logical, fill = FALSE)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Correct.

\textbf{22/10/19 - 8:52pm}

\textbf{Objective:} Find out how many months (on average) were respondents without food if they did belong to an irrigation association?

\textbf{Action:} Used the code \begin{verbatim}
    interviews_months_lack_food %>%
  mutate(number_months = rowSums(select(., Apr:Sept))) %>%
  group_by(memb_assoc) %>%
  summarize(mean_months = mean(number_months))
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success. R returned with \begin{verbatim}
    # A tibble: 3 x 2
  memb_assoc mean_months
  <chr>            <dbl>
1 no                2.31
2 yes               2.64
3 NA                2.95
\end{verbatim}

\subsection{Building your plots iteratively}

\textbf{29/10/19 - 1:23pm}

\textbf{Objective:} Use what you just learned to create a scatter plot of rooms by village with the respondent\_wall\_type showing in different colors.

\textbf{Action:} Used the code \begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = village, y = rooms)) +
    geom_jitter(aes(color = respondent_wall_type))
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} 

\includegraphics[width=\textwidth]{ggplot_5.png}

\subsection{Boxplot}

\textbf{29/10/19 - 1:29pm}

\textbf{Objective:} Replace the box plot with a violin plot.

\textbf{Action:} Used the code \begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type, y = rooms)) +
  geom_violin(alpha = 0) +
  geom_jitter(alpha = 0.5, color = "tomato")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} 

\includegraphics[width=\textwidth]{ggplot_8.png}

\textbf{29/10/19 - 1:31pm}

\textbf{Objective:} Create a boxplot for liv\_count for each wall type. Overlay the boxplot layer on a jitter layer to show actual measurements.

\textbf{Action:} Used the code \begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type, y = liv_count)) +
  geom_boxplot(alpha = 0) +
  geom_jitter(alpha = 0.5)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} 

\includegraphics[width=\textwidth]{ggplot_9.png}

\textbf{29/10/19 - 1:34pm}

\textbf{Objective:} Add color to the data points on your boxplot according to whether the respondent is a member of an irrigation association.

\textbf{Action:} Used \begin{verbatim}
    ggplot(data = interviews_plotting, aes(x = respondent_wall_type, y = liv_count)) +
  geom_boxplot(alpha = 0) +
  geom_jitter(aes(alpha = 0.5, color = memb_assoc))
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} 

\includegraphics[width=\textwidth]{ggplot_10.png}

\subsection{Barplots}

\textbf{29/10/19 - 1:58pm}

\textbf{Objective:} Create a bar plot showing the proportion of respondents in each village who are or are not part of an irrigation association.

\textbf{Action:} Used the code \begin{verbatim}
    percent_memb_assoc <- interviews_plotting %>%
  filter(!is.na(memb_assoc)) %>%
  count(village, memb_assoc) %>%
  group_by(village) %>%
  mutate(percent = n / sum(n)) %>%
  ungroup()
\end{verbatim}

then

\begin{verbatim}
    ggplot(percent_memb_assoc, aes(x = village, y = percent, fill = memb_assoc)) +
geom_bar(stat = "identity", position = "dodge")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:}

\includegraphics[width=\textwidth]{ggplot_15.png}

\subsection{ggplot2 themes}

\textbf{29/10/19 - 2:09pm}

I think I like the minimal theme the best.

\includegraphics[width=\textwidth]{ggplot_20.png}

\subsection{Customization}

This is more my vibe.


\includegraphics[width=\textwidth]{ggplot_25.png}

\newpage
\section{R Error List}

\subsection{Mutate}

\textbf{22/10/19 - 6:44pm}

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared.

\textbf{22/10/19 - 6:50pm}

Trying again.

\textbf{Objective:} Create a new data frame from the interviews data that meets the following criteria: contains only the village column and a new column called total\_meals containing a value that is equal to the total number of meals served in the household per day on average (no\_membrs times no\_meals). Only the rows where total\_meals is greater than 20 should be shown in the final data frame.

\textbf{Action:} Used the code and entered each line individually this time. \begin{verbatim}
    interviews_total_meals <- interviews %>%
    mutate(total_meals = no_membrs * no_meals) %>%
    filter(total_meals > 20) %>%
    select(village, total_meals)
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Nothing appeared. I don't get it. I'm moving on.

EDIT: It did work. I typed in \begin{verbatim}
    interviews_total_meals
\end{verbatim} and it had shown that contents of the object. Therefore it was user error.

\newpage
\section{Proof of Concept}

\subsection{General Thoughts}

\textbf{13/9/19 - 5:21pm}

I have received back my marks for the Elaboration Results and it has left me disheartened. I think now more than ever am I unsure about what I am doing, whether I should be aiming to stretch myself and try something ambitious or just design something that will work and assist me in my research. Honestly, I am leaning towards the latter.

Two main points of feedback for my Elaboration results are:
\begin{enumerate}
    \item I didn't detail alternatives to Voyant Tools
    \item That simply going through the articles I find in \textit{Neues Deutschland} and copying and pasting them into .txt files is not the best way to move forward
\end{enumerate}

I accept the feedback regarding the first point. I should have detailed alternatives to Voyant Tools for my textual analysis. As for the second, I would rather not automate that process for two main reasons.
\begin{enumerate}
    \item If I were to somehow automate the process of copying text from the \textit{Neues Deutschland} website, it would also copy in advertisements and other non-relevant text that would be too much of a hassle to clean up after the fact.
    \item Many results returned for the search query are not relevant at all to my research. This would only serve to save more unnecessary articles on my machine, which could skew corpus analysis at a later date. I would feel more comfortable actually \textbf{reading} every article on \textit{Neues Deutschland} before choosing which I should save and which I would not.
\end{enumerate}

\textbf{16/9/19 - 3:15pm}

Having a look at the next step for the Proof of Concept and I am finding it difficult ascertaining what I should actually be doing. It seems like the user stories that I have come up with are very similar, if not the same as the steps I had covered in Elaboration? Do I just repeat that work?

\textbf{16/9/19 - 4:02pm}

Without receiving feedback on my Proof of Concept Design with user stories, I am going to go ahead with what I have come up with. I am making the point that the collection of articles has already been done. My Proof of Concept will only be concerned with the analysis of text and storage of data.

\textbf{16/9/19 - 4:44pm}

Having completed learning how to save the textual analysis performed by Voyant Tools as a .tsv file, I now realise it is in a format that can work with OpenRefine. I may now consider using OpenRefine in my workflow to analyse the data once more.

\textbf{19/9/19 - 10:23am}

I think I need to read more about Open Semantic Desktop Search. It seems like a very powerful piece of software. For now, I am happy that it is able to do the tasks I need it to do, but I have no doubt that it probably can do so much more.

\textbf{9/10/19 - 10:23am}

After clearing some of my uni workload, I was finally able to organise a meeting with Brian. The meeting proved to be very helpful, with Brian providing me clarification on what I needed to do as part of my Proof of Concept.

My main take away from this was that the point of the Proof of Concept was to get the computer to do more work where it can, giving me the opportunity to drink more coffee (or another beverage of my choice). I realised my Proof of Concept was automating the process of getting text analysed using Voyant Tools and straight into Open Semantic Desktop Search.

I have downloaded the Voyant Tools open-source software directly on my machine. Let's see what I can do with this.

\textbf{10/10/19 - 10:48pm}

So I finally got around to creating a GitHub repository for my Proof of Concept in the FOAR705 organisation. I feel like I'm playing catch up, but at least I know what I'm doing now.

\textbf{10/10/19 - 11:03pm}

I want to put in writing what I am actually working on as part of my Proof of Concept. What I want to do is to automate the process of \textbf{\textit{saving}} the results of textual analysis straight into Open Semantic Desktop Search. Having that tagged automatically would be great too.

\textbf{10/10/19 - 11:21pm}

I have downloaded Duplicati because it seems like a decent enough back up solution. I will look into it much further later.

\textbf{11/10/19 - 1:44pm}

Have sat in the consultation hour before class seeing if I could figure out how to output data from Voyant. Still no idea.

\textbf{11/10/19 - 4:30pm}

Just stayed back after class to work with Aaron Hammond trying to find the elusive Voyant API. Had no luck. Instead, I made a Slack channel called \#voyanttools to pool together students who are using Voyant in their workflow.

\textbf{16/10/19 - 2:10pm}

I have met with fellow student Sophie Avard to see where we align in our Proof of Concepts. We have both tried to find the Voyant API. We both have had no luck with this. I'm starting to wonder if it exists? Some documentation on GitHub keep pointing to "Trombone" as a backend into Voyant, but I still don't know how this works.

\textbf{17/10/19 - 6:40pm}

A few things have transpired since the last update.
\begin{itemize}
    \item First, and perhaps most importantly. I \emph{finally} have access to \textit{Neues Deutschland's} online archive. This means I am now able to see what kind of text I am able to save. The website offers text picked up by their own OCR technology, along with high resolution scans of the newspaper pages. Since the text they provide comes from OCR, there have been instances of errors that appear in the text. I am therefore more resolved to clean the data at the front end by visually confirming whether the text from the OCR matches the high resolution page scans. Whilst this process may seem tedious, I cannot envisage of a process that would automate this. It simply requires a human eye to confirm the data is correct.
    \item I also realised that I may need to create another user story. When the text is saved in a .txt file, it saves not only the title, but the subtitle, date and tagline information. This user story would then help with saving the articles in a format that would allocate a line for each type of information. My proposal would be saving the .txt files in this format
    \begin{verbatim}
        Date
        Article title
        Article subtitle
        Information on the author
        BLANK LINE
        Start of the body text
    \end{verbatim}
    This would help with textual analysis later on because I imagine I could write a script in Terminal that is able to copy selected .txt files and extract either the Body of the text, the article name, date, etc. This will mean that words found in the titles will not appear in the analysis of the body text. Or analysis of just titles will be possible.
    \item I still have not had any luck finding an API for Voyant. I feel like giving up on this front and focus instead on writing a script that will help me manipulate the .txt files.
\end{itemize}

\textbf{23/10/19 - 5:19pm}

I have spent the past two hours looking for a way to having Voyant automatically open up files I stored in corpus\_output. I have not found anything. I feel like I'm wasting my time with this.

\textbf{23/10/19 - 5:39pm}

I have stumbled on a helpful video tutorial that explains Textual Analysis in R. 

\url{https://www.youtube.com/watch?v=pFinlXYLZ-A}

I think I will forego use of Voyant and try to use R instead.

\textbf{24/10/19 - 10:26pm}

I think I'm nearing the end of my PoC testing. Now that I have created a bash script that automates the process of extracting body text and creating next .txt files in the corpus\_output/body/ directory, I am thinking if I can automate the R script as well. For this purpose, I will create a new user story that combines that of \textbf{Extract the specific information I want from the .txt files} and \textbf{Identify themes}. I will call this new story \textbf{Automate the extract and identify process}.

\newpage
\subsection{Proof of Concept Design - User Stories}

The majority of user stories below were conceived of during the Elaboration Phase. New user stories have since been conceived of since I have gained access to the source material I will be dealing with. I have reintroduced the user story of \textbf{Save sources} from Elaboration I, whilst I have added the new user story \textbf{Extract the specific information I want from the .txt files}.

\subsubsection{Save sources}

As a research student, I would like to save the text from the newspaper articles into a .txt format that allows for an easier extraction of the information I want.

\subsubsection{Extract the specific information I want from the .txt files}

As a research student, I would like to be able to run a script that is able to target specific sections of the text for analysis, i.e. titles, dates, the body of the text, authors, for more concise analysis.

\subsubsection{Identify Themes}

As a research student, I would like to use software that is able to analyse a corpus of text and identify emergent themes thereof.

\subsubsection{Automate the extract and identify process}

As mentioned in my General Thoughts section, I have created this new user story as I feel I can combine the two user stories of \textbf{Extract the specific information I want from the .txt files} and \textbf{Identify Themes}.

As a research student, I would like to be able to automate the process of extracting specific information in the .txt files I have saved from \textit{Neues Deutschland} and identify themes therein.

\subsubsection{Store Analysis}

As a research student, I would like to store the data captured from the textual analysis onto my machine.

\subsubsection{Additional Notes}

As a research student, I would like to store any notes I have written about the analysis separate to the data captured so not to corrupt the original data.

\subsubsection{Create Tags}

As a research student, I would like to create a set of tags that will help me organise all my data (including articles, textual analysis data and research notes.

\subsubsection{Search My Research}

As a research student, I would like software that would allow me to search through all newspaper articles, data from textual analysis and notes.

\subsubsection{Tag Newspaper Articles}

As a research student, I would like software that would allow me to tag themes or keywords to the newspaper articles that have been analysed.

\subsubsection{Tag Analysis}

As a research student, I would like software that would allow me to tag themes or keywords to the data captured from the textual analysis.

\subsubsection{Tag Notes}

As a research student, I would like software that would allow me to tag themes or keywords to the notes that I have written as part of my research.

\newpage
\subsection{Proof of Concept - User Stories Testing}

\subsubsection{Save sources}

\subsubsection{Extract the specific information I want from the .txt files}

The first part of this step needs to be manually performed. I need to select which articles (in their .txt file format) will be part of a corpus that will be analysed. I will then copy the original files into a directory called 'corpus\_input'. From here I am testing if I can make a script that will extract whatever information I want out of the corpus and put it into a new directory called 'corpus\_output'.

\textbf{17/10/19 - 10:30pm}

\textbf{Objective:} Read only the title of the article in a .txt file.

Because the title of the article is on line 2 of the .txt file, I should be able to use a code to get this line.

\textbf{Action:} Using the following code: \begin{verbatim}
    sed -n 2p *.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returns with the result 'Unverantwortlicher Entscheid', which is the title of the first article. I'm not sure why it only return one .txt's title when I used the wildcard *.txt.

\textbf{17/10/19 - 10:45pm}

Maybe I should try the head and tail commands as suggested in the Unix SWC lesson.

\textbf{Objective:} Read only the title of the article in a .txt file using the head and tail commands in a loop.

\textbf{Action:} Used the following code:\begin{verbatim}
    for filename in 1973-11-13_UnverantwortlicherEntscheid.txt
    1973-11-21_FIFA-Haltungbefremdend.txt 
    do
    head -n 2 $filename | tail -n 1
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned \begin{verbatim}
    Unverantwortlicher Entscheid
    FIFA-Haltung befremdend
\end{verbatim} which are the titles of the two articles!

\textbf{17/10/19 - 10:55pm}

Now to see if I can work with variables to get the titles from \emph{all} the .txt files.

\textbf{Objective:} Extract all the titles from every single .txt in the corpus\_input directory.

\textbf{Action:} Used the commands:\begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned this \begin{verbatim}
Unverantwortlicher Entscheid
Australien qualifiziert
Kurz berichtet
Die falschen Garantien der chilenischen Junta
Bulgarien qualifiziert
Holland nach 0:0 gegen Belgien qualifiziert
FIFA-Haltung befremdend
Nun im Nepstadion Sieg für Nationalelf 1:0
\end{verbatim} which is correct!

\textbf{17/10/19 - 11:09pm}

Now to see if I can create a new text file with all titles in it.

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    cat $filename >> articletitles.txt
    done
\end{verbatim}

\textbf{Error:} Yes, and error! It didn't work exactly as planned. 

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files. I think this is because the \begin{verbatim}
    cat $filename >> articletitles.txt
\end{verbatim} command wasn't piped the the earlier \begin{verbatim}
    do head -n 2 $filename | tail -n 1
\end{verbatim} command.

\textbf{17/10/19 - 11:16pm}

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 | cat $filename >> articletitles.txt
    done\end{verbatim}

\textbf{Error:} Same error as before...

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files.

\textbf{17/10/19 - 11:38pm}

After conceptualising it step by step in my mind, I think I know the code I should use.

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    done > articletitles.txt \end{verbatim}

\textbf{Error:} None! Finally!

\textbf{Result:} articletitles.txt file created with ALL the titles from the articles put on their own line.

\textbf{17/10/19 - 11:46pm}

The last step is to move the newly created articletitles.txt file to the corpus\_output directory ready for analysis.

\textbf{Move the newly created articletitles.txt file to the corpus\_output directory}

\textbf{Action:} Use the following command:\begin{verbatim}
    mv articletitles.txt ../corpus_output/articletitles.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} articletitles.txt moved from corpus\_input directory to the corpus\_output directory.

\textbf{18/10/19 - 12:25pm}

Now that I have figured out how to extract just the title, I want to see if I can write a command that will extract the title and the subtitle from a .txt file.

\textbf{Objective:} Extract the title and the subtitle from a .txt file.

\textbf{Action:} Use the code: \begin{verbatim}
    head -n 3 1973-11-13_UnverantwortlicherEntscheid.txt | tail -n 2
\end{verbatim} (because I know this article definitely has a subtitle)

\textbf{Error:} None.

\textbf{Result:} Terminal returned this \begin{verbatim}
    Unverantwortlicher Entscheid
    Generalsekretariat der FIFA brüskiert die Weltöffentlichkeit
\end{verbatim} which is the title and subtitle. So success!

\textbf{18/10/19 - 12:28pm}

Now to see if I can do it as a loop to target all the .txt files in the corpus\_input directory.

\textbf{Objective:} Extract the title and the subtitle from all .txt files in the corpus\_input directory.

\textbf{Action:} Used the code: \begin{verbatim}
    for filename in *.txt
    do head -n 3 $filename | tail -n 2
    done \end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned this \begin{verbatim}
    Unverantwortlicher Entscheid
    Generalsekretariat der FIFA brüskiert die Weltöffentlichkeit
    Australien qualifiziert

    Kurz berichtet

    Die falschen Garantien der chilenischen Junta

    Bulgarien qualifiziert

    Holland nach 0:0 gegen Belgien qualifiziert

    FIFA-Haltung befremdend
    TASS: Augen vor den bekannten Tatsachen verschlossen
    Nun im Nepstadion Sieg für Nationalelf 1:0
    Die Entscheidung fiel durch ein Kopfballtor von Lauck
\end{verbatim} which are the titles and subtitles from all the articles in the corpus\_input directory. Another success!

\textbf{18/10/19 - 12:34pm}

Last step for the titles and subtitles.

\textbf{Objective:} Create a new text file filled with all the titles of the articles and move that file to the corpus\_output directory.

\textbf{Action:} Use the code: \begin{verbatim}
    for filename in *.txt
    do head -n 3 $filename | tail -n 2
    done > articletitlessubtitles.txt
    mv articletitlessubtitles.txt ../corpus_output/articlestitlessubtitles.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success! A new file called articlestitlessubtitles.txt was created in the corpus\_output directory. This .txt file contains all the titles and subtitles from the newspaper articles.

\textbf{18/10/19 - 1:25pm}

I am currently at consultation hours to ask Brian some questions. After being initially happy with what I did last night regarding Terminal, I realised I need to create the same number of .txt files that I have from the input. This will help with better analysis in Voyant.

\textbf{Objective:} Extract titles from the .txt files in corpus\_input and create corresponding files in corpus\_output with just the titles.

\textbf{Action:} Used the code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 > ../corpus_output/title-${filename}
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success! New .txt files created in corpus\_output that contain just the titles.

\textbf{18/10/19 - 1:35pm}

Taking a step back, I now realised I may need to make mroe directories that are more specific with the output it contains. I may need to develop commands that creates these directories.

\textbf{Objective:} Write commands that create the directories necessary for this PoC.

\textbf{Action:} Use the code: \begin{verbatim}
    mkdir data
    mkdir data/corpus_input/
    mkdir data/corpus_output/
    mkdir data/corpus_output/titles/
    mkdir data/corpus_output/titlessubtitles/
    mkdir data/corpus_output/body/
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success! All directories were created. This step would probably be the first thing that needs to happen.

\textbf{18/10/19 - 1:45pm}

\textbf{Objective:} Create .txt files in the corpus\_output/titles/ directory that contain titles only.

\textbf{Action:} Back in the corpus\_input directory. Use the code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 > ../corpus_output/titles/title-${filename}
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success. .txt files created in the corpus\_output/titles/ directory.

\textbf{18/10/19 - 5:00pm}

\textbf{Objective:} Create .txt files in the corpus\_output/titlessubtitles/ directory that contain titles and subtitles.

\textbf{Action:} Use the code: \begin{verbatim}
    for filename in *.txt
    do head -n 3 $filename | tail -n 2 >
    ../corpus_output/titlessubtitles/titlesubtitles-${filename}
    done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Succcess. Objective achieved.

\textbf{23/10/19 - 1:13pm}

\textbf{Objective} Extract the body of the articles. Will attempt this with one article first.

\textbf{Action:} Used the code \begin{verbatim}
    tail -n +6 1973-11-13_UnverantwortlicherEntscheid.txt 
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned this \begin{verbatim}
Berlin (ADN). Einen unverantwortlichen und allen moralischen Prinzipien
hohnsprechenden Entscheid traf am Montag das Generalsekretariat der
Internationalen Fußballföderation (FIFA). Wie aus einer Presseerklärung
des Generalsekretariats der FIFA hervorgeht, brüskierte der
Weltfußballverband die berechtigte Forderung des sowjetischen
Fußballverbandes, vieler Sportler und der Weltöffentlichkeit, das
Weltmeisterschafts-Qualifikationsspiel Chile gegen Sowjetunion in einem
anderen Lande auszutragen, erklärte Chile zum' Endrundenteilnehmer an der
Weltmeisterschaft und drohte dem sowjetischen Verband darüber hinaus
weitere Strafen an. Damit stellt sich das Generalsekretariat der FIFA
gegen die in einer weltweiten Protestbewegung Zum Ausdruck gekommene
internationale Meinung, daß es für die Mannschaft der Sowjetunion wie für
die anderer Länder aus menschlichen und moralischen Gründen unvereinbar
ist, auf der mit dem Blut chilenischer Patrioten getränkten Erde des
Andenlandes Fußball zu spielen. Die Erklärung richtet sich Zugleich gegen
die Meinung einer Reihe von nationalen Fußballverbänden — so dem
Fußballverband der DDR —, die sich mit der Haltung der Sowjetunion
solidarisch erklärt haben. Die Weltöffentlichkeit erwartet, daß das
Exekutivkomitee der FIFA sofort zusammentritt und diesen Entscheid des
Generalsekretariats aufhebt.
\end{verbatim}

That is the BODY only and not the other bits of information. So it seems that is a success.

\textbf{23/10/19 - 2:05pm}

\textbf{Objective:} Try using the code with another .txt file to see if it works.

\textbf{Action:} Used the code \begin{verbatim}
    tail -n +6 1973-11-19_Hollandnach0-0gegenBelgienqualifiziert.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Terminal returned this. \begin{verbatim}
Durch ein 0:0-Unentschieden gegen Belgien im letzten Spiel der
Europagruppe 3 qualifizierte sich die niederländische Nationalmannschaft
am Sonntag in Amsterdam lediglich durch das bessere Torverhältnis für die
Endrunde der Fußball-Weltmeisterschaft 1974. Bereits im ersten Treffen
beider Länder war kein Tor gefallen.
1. Niederlande 24:2 10:2
2. Belgien 13:0 10:2
3. Norwegen 9:16 4:8
4. Island 2:29 0:12
Bulgariens Fußball-Nationalmannschaft, die bereits seit Mittwoch als
Endrundenteilnehmer feststeht, besiegte am Sonntag in Sofia im letzten
Spiel der WM- Qualifikationsgruppe 6 Zypern mit 2:0 (1:0) Toren. Die
Treffer der Gastgeber erzielten Konew (2.) und Denew (68.).
1. Bulgarien 13:3 10:2
2. Portugal 10:8 7:5
3. Nordirland 5:6 5:7
4. Zypern 1:14 2:10
In der Europagruppe 2 gewann die Türkei gegen die Schweiz mit 2:0, jedoch
stand Italien bereits als Endrundenteilnehmer fest.
1. Italien 12:0 10:2
2. Türkei 5:3 6:6
3. Schweiz 2:4 6:6
4. Luxemburg 2:14 2:10
In der Afrika-Endrunde besiegte Zaire am Sonntag in Kinshasa die Auswahl
Sambias mit 2:1 (1:1) Toren. Die Gastgeber hatten bereits das erste
Treffen gegen Sambia mit 2:0 zu ihren Gunsten entschieden.
1. Zaire 4:1 4:0
2. Sambia 5:4 2:4
3. Marokko 0:4 0:2
\end{verbatim}

which is the BODY of that .txt file.

\textbf{23/10/19 - 2:10pm}

Now that I know it works on two separate files, I need to know if it will work with a very long article. I have created a new .txt file called lengthtest.txt which is a duplicate of one of the articles. I then copied the body four times to increase the length.

\textbf{Objective:} Run a stressor test with a long .txt file to see if it still works.

\textbf{Action:} Ran the code \begin{verbatim}
    tail -n +6 lengthtest.txt 
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} A very long block of text was returned. I won't paste the code here because its too long. As such, this test was a success and the code isn't broken with a long .txt file.

\textbf{23/10/19 - 2:17pm}

Now to see if the command can work in a loop to target all .txt files.

\textbf{Objective:} Use unix command to extract the body of each article and create new .txt files that are in the corpus\_output directory.

\textbf{Action:} Used the code \begin{verbatim}
    for filename in *.txt
do tail -n +6 $filename > ../corpus_output/body/body-${filename}
done
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} New .txt files in the /corpus\_output/body/ directory created that contain only the body text of their respective articles. SUCCESS!

\textbf{24/10/19 - 10:13pm}

The next step to make this whole process work more efficiently is to see if I can write a bash script to automate the process.

\textbf{Objective:} Create bin directory in directory structure

\textbf{Action:} Used the code \begin{verbatim}
    mkdir bin
\end{verbatim}
in my PoC test directory.

\textbf{Error:} None.

\textbf{Result:} bin directory created.

\textbf{24/10/19 - 10:17pm}

Now to see if I can write a bash script that automates the process of extracting the body text from the articles.

\textbf{Objective:} Write bash script to automate the process of extracting the body text of the articles and save it in the bin directory

\textbf{Action} Started with the code \begin{verbatim}
    for filename in *.txt
do tail -n +6 $filename > ../corpus_output/body/body-${filename}
done
\end{verbatim}

and modified it to

\begin{verbatim}
    #!/bin/bash

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do tail -n +6 $filename > ../corpus_output/body/body-${filename}
done
\end{verbatim}

I added the change in directory because Terminal will need to leave the bin folder and open the corpus\_input folder to get the .txt files. I also changed the extension to be .sh

\textbf{Error:} None.

\textbf{Result:} extractbody file in bin directory.

\textbf{24/10/19 - 10:19pm}

Now to test the bash script.

\textbf{Objective:} Test bash script in bin folder.

\textbf{Action:} \begin{itemize}
    \item Opened Terminal
    \item Navigated to the PoC directory
    \item Used the code \begin{verbatim}
        chmod 700 extractbody.sh to make it a executable file
    \end{verbatim}
\item Typed the code \begin{verbatim}
    bash extractbody.sh
\end{verbatim}
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} It automated the process! I'm quite proud of myself.

\subsubsection{Identify Themes}

\textbf{16/9/19 - 4:05pm}

\textbf{Objective:} See if Voyant Tools is able to analyse a corpus of text to see if there are any themes that emerge.

\textbf{Action:}
\begin{enumerate}
    \item Go to \url{www.voyant-tools.org}
    \item Select upload
    \item Navigate to the directory on my machine where the corpus is selected
    \item Select all the .txt files that I want analysed
    \item Select reveal
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Voyant Tools successfully analyses a corpus of text.

\textbf{10/10/19 - 11:40pm}

\textbf{Objective:} See if I can run Voyant Tools on my computer.

\textbf{Action:}

\begin{itemize}
    \item Downloaded Voyant Tools .zip file from Voyant Server website
    \item Unpackaged .zip file
    \item Run VoyantServer.jar
\end{itemize}

\textbf{Error:} VoyantServer.jar would not open

\textbf{Result:} VoyantServer.jar wouldn't run because it needs to install Java Development Kit to run.

\textbf{10/10/19 - 11:49pm}

\textbf{Objective:} See if I can run Voyant Tools on my computer after installing JDK.

\textbf{Action:}

\begin{itemize}
    \item Downloaded Voyant Tools .zip file from Voyant Server website
    \item Unpackaged .zip file
    \item Run VoyantServer.jar
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} VoyantServer is running. It also opens up a window in my default browser.

\includegraphics[width=\textwidth]{voyantserver.png}

\textbf{23/10/19 - 3:13pm}

Now that I have Voyant Server running on my machine, I want to see if I can change the Voyant Server settings so that it will automatically open a directory and use .txt files wherein to make a new corpus.

\textbf{Objective:} Change serversettings.txt in the Voyant Server data folder to point to the corpus\_output/body/ directory to open up the body .txt files.

\textbf{Action:} Changed \begin{verbatim}
    uri_path = 
\end{verbatim}

to \begin{verbatim}
    uri_path = /?input=/Users/janakin/Documents/PoC/data/corpus_output/body/
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Voyant server opened up an analysis window, but only analysed the path itself, i.e. it analysed janakin, Documents, PoC, data, corpus, output and body.

\textbf{23/10/19 - 9:36pm}

After doing some reading around on various tutorials on numerous websites, I think I will try to use R for textual analysis instead of Voyant.

\textbf{Objective:} Create new RStudio Project

\textbf{Action:} Created 'textualanalysis' project in the PoC directory.

\textbf{Error:} None.

\textbf{Result:} textualanalysis.Rproj created in PoC directory.

\textbf{23/10/19 - 9:39pm}

\textbf{Objective:} Create new R script and save it as textualanalyis.R

\textbf{Action:} New File - R Script. Saved R Script as textualanalysis.r in PoC/textualanalysis/

\textbf{Error:} None.

\textbf{Result:} R Script textualanalysis.r saved in PoC/textualanalysis/

\textbf{23/10/19 - 9:43pm}

From the tutorials I read, I will need to use the tm and wordcloud packages.

\textbf{Objective:} Install and load tm and wordcloud libraries.

\textbf{Action:} Used the code

\begin{verbatim}
    if(!require(tm)){
  install.packages("tm")
  library(tm)
}
if(!require(wordcloud)){
  install.packages("wordcloud")
  library(wordcloud)
}
\end{verbatim}

\textbf{Error:} None so far?

\textbf{Result:} Code installs and loads packages.

\textbf{23/10/19 - 9:47pm}

Next step is to import the .txt files into R

\textbf{Objective:} Import .txt files into R

\textbf{Action:} Used code to locate the filepath folder \begin{verbatim}
folder <- "/Users/janakin/Documents/PoC/data/corpus_output/body"
\end{verbatim}

then used \begin{verbatim}
    filelist <- list.files(path=folder, pattern="*.txt")
\end{verbatim}

then used to link the file path with the folder \begin{verbatim}
    filelist <- paste(folder, "/", filelist, sep="")
\end{verbatim}

then tested to see if it worked by using \begin{verbatim}
    filelist
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} R console returned this \begin{verbatim}
[1] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-13_UnverantwortlicherEntscheid.txt"             
[2] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-14_Australienqualifiziert.txt"                  
[3] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-14_Kurzberichtet.txt"                           
[4] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-15_DiefalschenGarantienderchilenischenJunta.txt"
[5] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-16_Bulgartonqualifiziert.txt"                   
[6] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-19_Hollandnach0-0gegenBelgienqualifiziert.txt"  
[7] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-21_FIFA-Haltungbefremdend.txt"                  
[8] "/Users/janakin/Documents/PoC/data/corpus_output/body/
body-1973-11-22_NunimNepstadionSiegfürNationalelf1-0.txt"  
\end{verbatim}

Which seems like it worked.

\textbf{23/10/19 - 9:54pm}

Now to get R to read the text from the .txt files.

\textbf{Objective:} Get R to read the text from the .txt files.

\textbf{Action:} Used the code \begin{verbatim}
    readtext <- lapply(filelist, FUN = readLines)
\end{verbatim}

\textbf{Error:} Some. \begin{verbatim}
    Warning messages:
1: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-14_Australienqualifiziert.txt'
2: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-14_Kurzberichtet.txt'
3: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-15_DiefalschenGarantienderchilenischenJunta.txt'
4: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-16_Bulgartonqualifiziert.txt'
\end{verbatim}

However, the tutorial I read said that this would happen because some of the .txt files may have not finished with an ENTER command.

\textbf{23/10/19 - 10:06pm}

\textbf{Objective:} Collapse lines in the list into one big element. 

\textbf{Action:} Used code \begin{verbatim}
    corpus <- lapply(readtext, FUN = paste, collapse=" ")
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Elements collapsed successfully!

\textbf{23/10/19 - 10:17pm}

Now to convert the object corpus to something the tm package can use.

\textbf{Objective:} Convert the object corpus to something the tm package can use.

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus <- Corpus(VectorSource(corpus))
\end{verbatim}

Used \begin{verbatim}
    VCorpus
\end{verbatim} to test.

\textbf{Error:} None.

\textbf{Result:} R console returned this \begin{verbatim}
    <<SimpleCorpus>>
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 8
\end{verbatim} so it seems it is successful.

\textbf{23/10/19 - 10:25pm}

\textbf{Objective:} Create new corpus object and remove whitespace.

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus, stripWhitespace)
\end{verbatim}

\textbf{Error:} I think? \begin{verbatim}
    Warning message:
In tm_map.SimpleCorpus(VCorpus, stripWhitespace) :
  transformation drops documents
\end{verbatim}

\textbf{Result:} Despite the warning message, it still stripped whitespace.

\textbf{23/10/19 - 10:29pm}

\textbf{Objective:} Convert corpus to lower case

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus, content_transformer(tolower))
\end{verbatim}

\textbf{Error:} Same thing as the white space \begin{verbatim}
    Warning message:
In tm_map.SimpleCorpus(VCorpus, content_transformer(tolower)) :
  transformation drops documents
\end{verbatim}

\textbf{Result:} Success despite warning message.

\textbf{23/10/19 - 10:34pm}

\textbf{Objective:} Remove German stop words

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus_clean, removeWords, stopwords("german"))
\end{verbatim}

\textbf{Error:} Same as the past two code lines.

\textbf{Result:} Success despite warning message.

\textbf{23/10/19 - 10:37pm}

\textbf{Objective:} Remove punctuation

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus_clean, removePunctuation)
\end{verbatim}

\textbf{Error:} Same as the past three code lines.

\textbf{Result:} Success despite warning message.

\textbf{23/10/19 - 10:40pm}

\textbf{Objective:} Create Term-Document Matrix so we can manipulate the data

\textbf{Action:} Used the code \begin{verbatim}
    dtm <- TermDocumentMatrix(VCorpus_clean)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} TermDocumentMatrix created.

\textbf{23/10/19 - 10:45pm}

\textbf{Objective:} Test dtm for what it returns

\textbf{Action:} Used the code \begin{verbatim}
    findFreqTerms(dtm, 5)
\end{verbatim}

Which will find terms that occur at least 5 times.

\textbf{Error:} None.

\textbf{Result:} R console returned this \begin{verbatim}
[1] "fifa"                    
[2] "fußballweltmeisterschaft"
[3] "spiel"                   
[4] "wurde"                   
[5] """ (The upside down German version)                       
[6] "auswahl"                 
[7] "ungarischen" 
\end{verbatim}

I think """ (The upside down German version) appeared because it's not recognised as punctuation in English.

\textbf{23/10/19 - 11:00pm}

\textbf{Objective:} Remove """ (The upside down German version) from corpus

\textbf{Action:} Entered the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus_clean, removeWords, c("""))
\end{verbatim} into the script.

\textbf{Error:} It didn't remove the """ (The upside down German version)

\textbf{Result:} It didn't remove the """ (The upside down German version). I'll just move on for now and see if I can fix it later.

\textbf{23/10/19 - 11:04pm}

\textbf{Objective:} To get list of frequent words

\textbf{Action:} Used the code \begin{verbatim}
    m <- as.matrix(dtm)
    v <- sort(rowSums(m), decreasing=TRUE)
    d <- data.frame(word = names(v), freq=v)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Success.

\textbf{23/10/19 - 11:07pm}

\textbf{Objective:} Find the top 20 frequent words that occur a minimum 2 times.

\textbf{Action:} Used the code \begin{verbatim}
    head(d,20, min.freq=2)
\end{verbatim}

\subsubsection{Automate the extract and identify process}

As mentioned in my General Thoughts section, I have created this new user story as I feel I can combine the two user stories of \textbf{Extract the specific information I want from the .txt files} and \textbf{Identify Themes}.

\textbf{24/10/19 - 10:37pm}

Now that I have a bash script which automates the extract body text process, I need to complete this R script so that it will continue the automation and output some word frequency data. Before I output any data, I need to create a data frame using R so I can do the outputing.

\textbf{Objective:} Create a data.frame for the VCorpus\_clean

\textbf{Action:} Used the code \begin{verbatim}
    m <- as.matrix(dtm)
    v <- sort(rowSums(m), decreasing=TRUE)
    d <- data.frame(word = names(v), freq=v)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} dataframe d created for VCorpus\_clean.

\textbf{24/10/19 - 10:51pm}

\textbf{Objective:} Write .csv file from the corpus dataframe

\textbf{Action:} Used the code \begin{verbatim}
    write.csv(d, "corpuswordfrequency.csv", row.names=FALSE)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} corpuswordfrequency.csv file created in the same directory where textualanalysis.R is stored.

\textbf{24/10/19 - 10:56pm}

\textbf{Objective:} Write code to save outputed dataframe .csv file in the directory /data/wordfreq/

\textbf{Action:} Created wordfreq directory in Poc folder. Then wrote the code \begin{verbatim}
    write.csv(d, file="/users/Janakin/Documents/PoC/data/wordfreq/
    corpuswordfrequency.csv", row.names=FALSE)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} corpuswordfrequency.csv file created in wordfreq directory!

\textbf{24/10/19 - 10:59pm}

Now for the big test. I want to see if I can get my bash script to run my R script as well.

\textbf{Objective:} Modify extractbody.sh to run the R script that creates a .csv file from the word frequency found in a corpus.

\textbf{Action:} \begin{itemize}
    \item Copied textualanalysis.R
    \item Pasted textualanalysis.R in Poc/bin/
    \item Renamed textualanalysis to wordfreq\_body.R
    \item Rewrote extractbody.sh to \begin{verbatim}
        #!/bin/bash

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do tail -n +6 $filename > ../corpus_output/body/body-${filename}
done

# Go back to the bin folder to run R script
cd ../..
cd bin
Rscript wordfreq_body.R
    \end{verbatim}
    \item Ran bash extractbody.sh
\end{itemize}

\textbf{Error:} Yes

\textbf{Result:} This was the error message in Terminal \begin{verbatim}
    Error in file(con, "r") : cannot open the connection
\end{verbatim}

I will revisit this later.

\textbf{24/10/19 - 11:30pm}

I just realised my R code will overwrite any existing .csv in the wordfreq directory.

\textbf{Objective:} Modify R script to ensure it does not overwrite existing .csv files in wordfreq directory.

\textbf{Action:} Used the code \begin{verbatim}
    write.csv(d, file=paste0("/users/Janakin/Documents/PoC/data/
    wordfreq/corpuswordfrequency-", Sys.time(), ".csv"), row.names=FALSE)
\end{verbatim}

\textbf{25/10/19 - 12:11am}

Revisiting my attempt to synthesise Extract and Identify. After some head scratching, I realised I was getting that error because the R Script was looking for files that were not there. I had deleted them to test another thing. I had to run my Terminal code again to make it appear.

\textbf{Objective:} Modify bash script to incorporate R Script.

\textbf{Action:} \begin{itemize}
    \item Rewrote code for wordfreq\_body.R and removed the install command for Wordcloud as I was not using that anymore.
    \item Ran extractbody.sh again
\end{itemize}

\textbf{Error:} None!

\textbf{Result:} Success! Process from start to finish is now automated.

\textbf{25/10/19 - 12:26am}

Now that I have automated the process for body text, I will see if I can do the same for titles and titlessubtitles.

\textbf{Objective:} Automate process for titles.

\textbf{Action:} \begin{itemize}
    \item Created a extracttitles.sh file in PoC/bin directory
    \item Wrote the code \begin{verbatim}
        #!/bin/bash

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do head -n 2 $filename | tail -n 1 > ../corpus_output/titles/title-${filename}
done

# Go back to the bin folder to run R script
cd ../..
cd bin
Rscript wordfreq_titles.R
    \end{verbatim}
    \item Created a new R script called wordfreq\_titles.R in PoC/bin directory
    \item Created three new directories in the PoC/data/wordfreq/ directory. body, titles and titlessubtitles
    \item Used the code \begin{verbatim}
        # Package and library required for this work
if(!require(tm)){
  install.packages("tm")
  library(tm)
}
# The file path to where the .txt files are stored
folder <- "~/Documents/PoC/data/corpus_output/titles"
# Lists the files in the path
filelist <- list.files(path=folder, pattern="*.txt")
# Creates a path to all .txt files in path folder
filelist <- paste(folder, "/", filelist, sep="")
# Reads text from .txt files
readtext <- lapply(filelist, FUN = readLines)
# Collapses elements into one element
corpus <- lapply(readtext, FUN = paste, collapse=" ")
# Convert corpus to something tm package can use
VCorpus <- Corpus(VectorSource(corpus))
# Using a new corpus name 'VCorpus_clean to keep the original 
VCorpus object untouched
# Strip whitespace from corpus
VCorpus_clean <- tm_map(VCorpus, stripWhitespace)
# Convert corpus to lower case
VCorpus_clean <- tm_map(VCorpus_clean, content_transformer(tolower))
# Remove German stop words
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, stopwords("german"))
# Remove punctuation
VCorpus_clean <- tm_map(VCorpus_clean, removePunctuation)
# Remove """ and "—"from corpus
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, c(""","—"))
# Create Term-Document Matrix
dtm <- TermDocumentMatrix(VCorpus_clean)
# To get the list of frequency of words
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
# Output dataframe as a .csv file
write.csv(d, file=paste0("/users/Janakin/Documents/PoC/data/wordfreq/titles/
titlewordfrequency-", Sys.time(), ".csv"), row.names=FALSE)
    \end{verbatim}

    \item Unlocked the extracttitles.sh in Finder,
    \item Made the extracttitles.sh executable by using the chmod 700 command in Terminal.
    \item Ran bash extracttitles.sh
\end{itemize}

\textbf{Error:} None!

\textbf{Result:} Another success!

\textbf{25/10/19 - 12:39am}

I need to rewrite the wordfreq\_body.R script so that it writes the .csv file to the correct location.

\textbf{Objective:} Rewrite the wordfreq\_body.R script so that it writes the .csv file to the correct location.

\textbf{Action:} Replaced the old write code with \begin{verbatim}
    write.csv(d, file=paste0("/users/Janakin/Documents/PoC/data/
    wordfreq/body/bodywordfrequency-", Sys.time(), ".csv"), row.names=FALSE)
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} Objective achieved.

\textbf{25/10/19 - 12:43am}

Now to automate the process for titles and subtitles.

\textbf{Action:} 
    \begin{itemize}
    \item Created extracttitlessubtitles.sh in PoC/bin
    \item Unlocked extracttitlessubtitles.sh in finder
    \item Used the code \begin{verbatim}
        #!/bin/bash

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do head -n 3 $filename | tail -n 2 > ../corpus_output/titlessubtitles/
titlesubtitles-${filename}
done

# Go back to the bin folder to run R script
cd ../..
cd bin
Rscript wordfreq_titlessubtitles.R
    \end{verbatim}
    \item Created wordfreq\_titlessubtitles.R in PoC/bin
    \item Used the code \begin{verbatim}
        # Package and library required for this work
if(!require(tm)){
  install.packages("tm")
  library(tm)
}
# The file path to where the .txt files are stored
folder <- "~/Documents/PoC/data/corpus_output/titlessubtitles"
# Lists the files in the path
filelist <- list.files(path=folder, pattern="*.txt")
# Creates a path to all .txt files in path folder
filelist <- paste(folder, "/", filelist, sep="")
# Reads text from .txt files
readtext <- lapply(filelist, FUN = readLines)
# Collapses elements into one element
corpus <- lapply(readtext, FUN = paste, collapse=" ")
# Convert corpus to something tm package can use
VCorpus <- Corpus(VectorSource(corpus))
# Using a new corpus name 'VCorpus_clean to keep the original 
VCorpus object untouched
# Strip whitespace from corpus
VCorpus_clean <- tm_map(VCorpus, stripWhitespace)
# Convert corpus to lower case
VCorpus_clean <- tm_map(VCorpus_clean, content_transformer(tolower))
# Remove German stop words
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, stopwords("german"))
# Remove punctuation
VCorpus_clean <- tm_map(VCorpus_clean, removePunctuation)
# Remove """ and "—"from corpus
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, c(""","—"))
# Create Term-Document Matrix
dtm <- TermDocumentMatrix(VCorpus_clean)
# To get the list of frequency of words
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
# Output dataframe as a .csv file
write.csv(d, file=paste0("/users/Janakin/Documents/PoC/data/wordfreq/
titlessubtitles/titlesubtitlewordfrequency-", 
Sys.time(), ".csv"), row.names=FALSE)
    \end{verbatim}
    \item Made extracttitlessubtitles.sh executable by using chmod 700 in Terminal.
    \item Ran bash extracttitlessubtitles.sh
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} bash script ran perfect. It extracted subtitles and titles and created a .csv of the most frequent words.

\textbf{25/10/19 - 12:56am}

\textbf{Objective:} Tidy up the path commands in the R script so that it uses the \begin{verbatim}
    ~
\end{verbatim} character instead of Users/name

\textbf{Action:} Change wherever there is /Users/Janakin/ to \begin{verbatim}
    ~
\end{verbatim} in all the R Scripts.

\textbf{Error:} None.

\textbf{Result:} Objective achieved.

\textbf{25/10/19 - 1:00am}

FINAL TEST. I want to see if I can write a bash script that will run extractbody.sh, extracttitles.sh and extracttitlessubtitles.sh.

\textbf{Objective:} Automate the entire process.

\textbf{Action:} \begin{itemize}
    \item Created go.sh in PoC/bin
    \item Wrote the code \begin{verbatim}
        #!/bin/bash

bash extractbody.sh
bash extracttitles.sh
bash extracttitlessubtitles.sh
    \end{verbatim}
    \item Unlocked go.sh in Finder
    \item Made go.sh executable using the chmod 700 command in Terminal
    \item Deleted all files in the subdirectories of PoC/data/wordfreq
    \item ran bash go.sh in Terminal
    \item Crossed my fingers.
\end{itemize}

\textbf{Error:} None!

\textbf{Result:} I THINK I HAVE DONE IT! Running one command and I have analysed the information I want!!! Thank you Brian, Shawn and Katherine!

\textbf{29/10/19 - 4:30pm}

I just realised, I forgot to remove the numbers from the corpus.

\textbf{Action:} Add code in R Script to remove numbers. \begin{verbatim}
    # Remove numbers
    VCorpus_clean <- tm_map(VCorpus_clean, removeNumbers)
\end{verbatim}. Ran go.sh again to see if numbers have been removed.

\textbf{Error:} None.

\textbf{Result:} Numbers have been removed.

\textbf{29/10/19 - 4:35pm}

Another I have realised. I probably should make a script that removes the data from the data/corpus\_output/ directories so that go.sh doesn't accidently add more .txt files, thereby skewing any wordfreq analysis.

\textbf{Objective:} Write a script that deletes .txt files

\textbf{Action:} Tested \begin{verbatim}
    rm *.txt
\end{verbatim}

\textbf{Error:} None.

\textbf{Result:} .txt files in the directory were deleted.

\textbf{29/10/19 - 4:38pm}

\textbf{Objective:} Write a bash script that deletes .txt files in the corpus\_output directories.

\textbf{Action:} Used the code to write a script \begin{verbatim}
    #!/bin/bash

# Changes directory to corpus_output/body directory
cd ..
cd data/corpus_output/body/

# Delete .txt files
rm *.txt

# Changes directory to corpus_output/titles directory
cd ../titles

# Delete .txt files
rm *.txt

# Changes directory to corpus_output/titlessubtitles directory
cd ../titlessubtitles/

# Delete .txt files
rm *.txt

# Changes directory to bin directory
cd ../../../bin/
\end{verbatim}. Saved it as delete.sh and chmod 700 the script to make it executable. Ran bash delete.sh in Terminal.

\textbf{Error:} None.

\textbf{Result:} delete.sh deleted the data out of the corpus\_output directories.

\textbf{3/11/19 - 11:53pm}

I have slightly amended some of the Terminal and R Scripts so that they now see 

\begin{verbatim}
    ~/corpus-analysis/
\end{verbatim}

instead of \begin{verbatim}
    ~/PoC/
\end{verbatim}

as the working directory. I thought I should give it a better name.

\textbf{3/11/19 - 11:54pm}

I have added \begin{verbatim}
    # Create output directories if needed
    mkdir -p ~/corpus-analysis/data/corpus_output/
    mkdir -p ~/corpus-analysis/data/corpus_output/body
    mkdir -p ~/corpus-analysis/data/wordfreq/
    mkdir -p ~/corpus-analysis/data/wordfreq/body
\end{verbatim} to the respective bash scripts so that they create the directories for output if they don't already exist.

\subsubsection{Store Analysis}

\textbf{16/9/19 - 4:24pm}

\textbf{Objective:} Find out what can be exported as data from Voyant Tools.

\textbf{Action:} Click on every 'Export' button to see what was applicable

\textbf{Error:} None.

\textbf{Result:} The following can be exported as .tsv files:
\begin{itemize}
    \item Terms
    \item Document Terms
    \item Documents
    \item Phrases
    \item Contexts
    \item Correlations
\end{itemize}

All other analysis outputs can be exported as Visualitions in the form of a .png file.

\textbf{16/9/19 - 4:37pm}

\textbf{Objective:} Store 'Terms' as a .tsv file
\begin{enumerate}
    \item Click on Export in the Terms window in Voyant Tools
    \item Click 'export all available data as tab separated values (text)'
    \item Copy the text from the new window that opens
    \item Open TextEdit on my machine
    \item Paste text into TextEdit
    \item Save as terms.tsv in the /Jugueta/MRes/data/textualanalysis/ directory
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} New filed saved as terms.tsv

\textbf{29/10/19 - 4:49pm}

Please read the user story above for how I chose to store analysis \emph{after} I changed my analysis software to R.

\subsubsection{Additional Notes}

\textbf{16/9/19 - 4:52pm}

\textbf{Objective:} Store additional notes I have made on the analysis onto my machine

\textbf{Action:}
\begin{enumerate}
    \item Open TextEdit
    \item Write notes
    \item Save notes a .txt file
    \item Save in the /Jugueta/MRes/notes/textualanalysis/ directory on my machine
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} Am able to use TextEdit to save notes on my machine.

\subsubsection{Create Tags}

\textbf{19/9/19 - 10:25am}

\textbf{Objective:} Create a 'stasi' tag that will be used to organise my data and notes relating to the East German Secret Police using Open Semantic Desktop Search.

\textbf{Action:}
\begin{enumerate}
    \item Launch VirtualBox VM
    \item Set the Shared Folder to /Jugueta/MRes/
    \item Start Open Semantic Desktop Search
    \item Click on the Activities menu in Open Semantic Desktop Search
    \item Click on Index documents to ensure that I am working with the right directory
    \item Click on Manage structure
    \item Click on Add new entry
    \item Enter 'stasi' for the name and select tag in Facet type
    \item Click Save
\end{enumerate}

\textbf{Error:} None.

\textbf{Result:} A 'stasi' tag is now in existence.

\subsubsection{Search My Research}

\textbf{19/9/19 - 10:59am}

\textbf{Objective:} Search for 'Hamburg' using Open Semantic Desktop Search

\textbf{Action:}
\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type in 'Hamburg' in the Search bar
    \item Click on Search
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} 38 results were returned with the query 'Hamburg'. This has searched my entire MRes folder though, so I want to see if there is a way to be more succinct with the search.

\textbf{19/9/19 - 11:37am}

\textbf{Objective:} Be more succinct with my searches, targeting a specific path or folder.

\textbf{Action:}
\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type in 'Hamburg' in the Search bar
    \item In Paths, select MRes
    \item Select notes
    \item Select articles
    \item Click on Search
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} Search was contained only in the articles folder. It returned 5 items.

\textbf{19/9/19 - 11:43am}

\textbf{Objective:} Figure out what is achievable in the search option settings.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Used numerous combinations of search phrases to test the search option settings.
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} Open Semantic Desktop Search has powerful bolean search operators. So it's just like MultiSearch. It also has fuzzy search options, that will return other word forms similar to the query (grammar and stemming).

\subsubsection{Tag Newspaper Articles}

\textbf{25/9/19 - 10:55am}

\textbf{Objective:} Tag 'stasi' to newspaper articles that are related to the Stasi.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type stasi in the search field
    \item Set the path to the newspaper directory
    \item Click on first returned result
    \item Confirm if relevant to Stasi
    \item Click the Tagging \& annotation button
    \item Go to the Tags tab
    \item Click on the stasi tag
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} The newspaper article is correctly tagged with the 'stasi' tag.

\subsubsection{Tag Textual Analysis}

\textbf{25/9/19 - 11:07am}

\textbf{Objective:} Tag 'stasi' to Voyant Tools Textual analysis data that are related to the Stasi.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type stasi in the search field
    \item Set the path to the textual analysis directory
    \item Click on first returned result
    \item Confirm if relevant to Stasi
    \item Click the Tagging \& annotation button
    \item Go to the Tags tab
    \item Click on the stasi tag
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} The data from the textual analysis is correctly tagged with the 'stasi' tag.

\subsubsection{Tag Notes}

\textbf{25/9/19 - 11:21am}

\textbf{Objective:} Tag 'stasi' to general research notes that I have written that are related to the Stasi.

\begin{itemize}
    \item Launch VirtualBox VM
    \item Start Open Semantic Desktop Search
    \item Type stasi in the search field
    \item Set the path to the notes directory
    \item Click on first returned result
    \item Confirm if relevant to Stasi
    \item Click the Tagging \& annotation button
    \item Go to the Tags tab
    \item Click on the stasi tag
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} The notes are correctly tagged with the 'stasi' tag.

\newpage
\subsection{Proof of Concept - Deployability Testing}

\textbf{25/10/19 - 12:30pm}

After thinking I had finished my PoC user testing I had fellow students Sophie Wallace and Tom Duloy to download and test my bash and R scripts. 

\textbf{Objective:} Test go.sh bash script.

\textbf{Action:} \begin{itemize}
    \item Downloaded PoC.zip from my Github repository.
    \item Unzipped PoC.zip to Documents folder.
    \item Ran go.bash from the bin directory in Terminal.
\end{itemize}

\textbf{Error:} Yes.

\textbf{Result:} This was the Terminal error message: \begin{verbatim}
    Error in contrib.url(repos, "source") : 
  trying to use CRAN without setting a mirror
Calls: install.packages -> contrib.url
In addition: Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘tm’
Execution halted
\end{verbatim}

\textbf{25/10/19 - 1:25pm}

After some reading on the internet, I found out that I needed to use a CRAN mirror if the files were to be downloaded outside an RStudio environment.

\textbf{Objective:} Rewrite RScript to use a CRAN mirror to download tm package.

\textbf{Action:} \begin{itemize}
    \item Used code \begin{verbatim}
    if(!require(tm)){
install.packages("tm",repos = "http://cran.us.r-project.org")
library(tm)
}
\end{verbatim} instead of the other code that would install packages from inside an RStudio environment.
    \item Ran extractbody.sh on Sophie's machine
\end{itemize}

\textbf{Error:} None.

\textbf{Result:} Success! So I will now replace the \begin{verbatim}
    install.packages("tm")
\end{verbatim} with \begin{verbatim}
    install.packages("tm",repos = "http://cran.us.r-project.org")
\end{verbatim} in all the RScripts.

\textbf{25/10/19 - 1:45pm}

\textbf{Objective:} See if go.sh runs on another persons machine.

\textbf{Action:} \begin{itemize}
    \item Re-downloaded the update R Scripts to Sophies machine
    \item Ran go.sh

\textbf{Error:} None.

\textbf{Result:} The software works. I think I finally have a fully functional 1.0 version.
\end{itemize}

\newpage
\subsection{Proof of Concept - Quality Assurance}

\subsubsection{Manual Testing}

\textbf{4/11/19 - 7:39pm}

To test the R Script that analyses the word frequency of terms found within a .txt corpus, I have selected Helene Fischer's song 'Atemlos durch die Nacht'. I have selected this example because it is in German. For the purposes of this test, I have selected the first verse and chorus to analyse. The lyrics are as follows.

\begin{verbatim}
Wir zieh'n durch die Straßen und die Clubs dieser Stadt
Das ist unsre Nacht, wie für uns beide gemacht, oho oho
Ich schließe meine Augen, lösche jedes Tabu
Küsse auf der Haut, so wie ein Liebes-Tattoo, oho, oho

Was das zwischen uns auch ist
Bilder die man nie vergisst
Und dein Blick hat mir gezeigt
Das ist unsre Zeit

Atemlos durch die Nacht
Bis ein neuer Tag erwacht
Atemlos einfach raus
Deine Augen zieh'n mich aus

Atemlos durch die Nacht
Spür' was Liebe mit uns macht
Atemlos, schwindelfrei
Großes Kino für uns zwei

Wir sind heute ewig, tausend Glücksgefühle
Alles was ich bin, teil' ich mit dir
Wir sind unzertrennlich, irgendwie unsterblich
Komm nimm meine Hand und geh mit mir
\end{verbatim}

Having counted all the words manually, I should expect the result: \begin{itemize}
    \item atemlos - 4
    \item oho - 4
    \item nacht - 3
    \item ziehn - 2
    \item unsre - 2
    \item augen - 2
\end{itemize}

All other words have a frequency of 1. 

Running the wordfreq\_body.R script, this is what was made.

\includegraphics[width=\textwidth]{unittest1.png}

A manual test confirms the script works.

\subsubsection{Automated Unit Testing}

\textbf{4/11/19 - 9:47pm}

I am trying unit testing in R. I have created a new Rproj file called R-unit-testing, along with copying and renaming my wordfreq\_body.R script. I have created two new R scripts: test\_script.R and run\_tests.R

Code for test\_script.R
\begin{verbatim}
testdf <- data.frame("word" = c("atemlos"), "freq" = 4)
dat <- head(d, n=1)
test_that("atemlos 4", {expect_identical(dat, testdf)})
\end{verbatim}

Code for run\_tests.R
\begin{verbatim}
install.packages("testthat")
library(testthat)
source("script.R")
test_results <- test_dir("test_script.R", reporter="summary")
\end{verbatim}

\textbf{Objective:} Automate unit testing.

\textbf{Action:} Ran run\_tests.R script

\textbf{Error:} Yes.

\textbf{Result:} R console returned this

\begin{verbatim}
    > library(testthat)
> source("script.R")
Loading required package: tm
Loading required package: NLP
Warning messages:
1: In FUN(X[[i]], ...) :
  incomplete final line found on '~/R-unit-testing/corpus_output/body/
    helenefischer-atemlos.txt'
2: In tm_map.SimpleCorpus(VCorpus, stripWhitespace) :
  transformation drops documents
3: In tm_map.SimpleCorpus(VCorpus_clean, content_transformer(tolower)) :
  transformation drops documents
4: In tm_map.SimpleCorpus(VCorpus_clean, removeWords, stopwords("german")) :
  transformation drops documents
5: In tm_map.SimpleCorpus(VCorpus_clean, removePunctuation) :
  transformation drops documents
6: In tm_map.SimpleCorpus(VCorpus_clean, removeWords, c(""", "—")) :
  transformation drops documents
> test_results <- test_dir("test_script.R", reporter="summary")
Error in test_files(paths, reporter = reporter, env = env, 
stop_on_failure = stop_on_failure,  : 
  No matching test file in dir
\end{verbatim}

I don't know what I'm doing wrong.

\textbf{4/11/19 - 9:56pm}

If you are interested, you can go to \url{https://github.com/MQ-FOAR705/Jugueta-PoC-WiP/tree/master/R-unit-testing} to see what I have tried.

\newpage
\subsection{Proof of Concept - Deployment Instructions}

\subsubsection{Instructions for Corpus Analysis Package}

- Download the \textbf{corpus-analysis} directory from this repository and move it into the ``User`` folder on your computer.

- Move your the \textbf{.txt} files that comprise your corpus into the \textbf{data/corpus\_input} directory. I have pre-loaded a corpus of German newspaper articles in this directory already in the event that you wish to test the workflow. If you are analysing a new corpus, please ensure that you remove all irrelevant \textbf{.txt} files from the \textbf{data/corpus\_input} folder, lest they are analysed as part of your corpus.

- Open your the Terminal program on your machine and navigate to the \textbf{corpus-analysis/bin} directory. If you have corrected placed the \textbf{corpus-analysis} folder in the \textbf{User} folder, then you should be able to use this Terminal code to navigate there.

\begin{verbatim}
    cd corpus-analysis/bin
\end{verbatim}

- If it is the first time you are running this package, then simply type in the command

\begin{verbatim}
    bash go.sh
\end{verbatim}

If it is not the first time you are running this package, then use the command \begin{verbatim}
    delete.sh
\end{verbatim} to clear the \textbf{data/corpus\_output} directories, reading for a clean run.

It will then analyse the corpus, outputting the data in a .csv format into the \textbf{data/wordfreq} directories. \begin{verbatim}
    bash go.sh
\end{verbatim} will analyse body text, titles and titles\&subtitles. If you want to analyse body text, titles or titles\&subtitles specifically, you can use the following codes.

\begin{verbatim}
    bash extractbody.sh
\end{verbatim} for body text only.
\begin{verbatim}
    bash extracttitles.sh
\end{verbatim} for titles only.
\begin{verbatim}
    bash extracttitlessubtitles.sh
\end{verbatim} for titles and subtitles only.

\textbf{Structure}

The \textbf{corpus-analysis} folder in the repository is the folder that contains all the necessary scripts and folders to run this Proof of Concept. Within the \textbf{corpus-analysis} are two subdirectories: \textbf{bin} and \textbf{data}. \textbf{bin} contains five shell bash scripts and three R scripts.

\textbf{Shell scripts}

\begin{verbatim}
    go.sh
\end{verbatim}

Is the primary shell script that triggers the entire process. 

\begin{verbatim}
    extractbody.sh
\end{verbatim}

Extracts text from line 6 (body text) down in the selected corpus and creates new \textbf{.txt} files in \textbf{data/corpus\_output/body} directory.

\begin{verbatim}
    extracttitles.sh
\end{verbatim}

Extracts text from line 2 (title text) in the selected corpus and creates new \textbf{.txt} files in \textbf{data/corpus\_output/titles} directory.

\begin{verbatim}
    extracttitlessubtitles.sh
\end{verbatim}

Extracts text from lines 2 and 3 (title text) in the selected corpus and creates new \textbf{.txt} files in \textbf{data/corpus\_output/titlessubtitles} directory.

\begin{verbatim}
    delete.sh
\end{verbatim}

Deletes \textbf{.txt} files in the \textbf{data/corpus\_output} directories.

\begin{verbatim}
    wordfreq\_body.R
\end{verbatim}

Analyses \textbf{.txt} files in \textbf{data/corpus\_output/body} directory and creates a list of words and their frequency in a \textbf{.csv} file located in the \textbf{data/wordfreq/body} directory.

\begin{verbatim}
    wordfreq_titles.R
\end{verbatim}

Analyses \textbf{.txt} files in \textbf{data/corpus\_output/titles} directory and creates a list of words and their frequency in a \textbf{.csv} file located in the \textbf{data/wordfreq/titles} directory.

\begin{verbatim}
    wordfreq_titlessubtitles.R
\end{verbatim}

Analyses \textbf{.txt} files in \textbf{data/corpus\_output/titlessubtitles} directory and creates a list of words and their frequency in a \textbf{.csv} file located in the \textbf{data/wordfreq/titlessubtitles} directory.

\textbf{Notes on Corpus Word Frequency Analysis for German Texts}

As mentioned in the title, this package was designed to analyse German language texts specifically. Analysing texts in English may still render some results, however, the R Script was written to identify German words.

\subsubsection{Reproducible RStudio Environment with renv.lock}

In the spirit of Open Source code, I have also provided users with access to the Rstudio Project file accessible here in the repository \url{https://github.com/MQ-FOAR705/jugueta-corpus-analysis/tree/master/textualanalysis}

The repository contains the .Rproj file, .R scripts and the renv.lock file that contains the packages 'tm' and 'slam' for redeployability.  For instructions on how to use the the renv.lock file, I have copied an excerpt from \url{https://rstudio.github.io/renv/articles/renv.html}

\begin{quote}Call renv::snapshot() to save the state of your project to a lockfile; and

Call renv::restore() to restore the state of your project from a lockfile.

Be aware that renv::restore() may fail if a package was originally installed through a CRAN-available binary, but that binary is no longer available. renv will attempt to install the package from sources in this situation, but attempts to install from source can (and often do) fail due to missing system prerequisites for compilation of a package. On Windows, the renv::equip() function may be useful – it will download external software commonly used when compiling R packages from sources, and instruct R to use that software during compilation.

By default, renv will maintain and use a global cache of packages during renv::restore(), so (at least on the same machine) if that cache is maintained old projects will be restored by copying or linking from an installation discovered in the cache.
\end{quote}


Please visit the link listed above for more information about renv.lock.

\newpage
\subsection{Proof of Concept - Disaster Recovery Plan}

\textbf{29/10/19 - 4:05pm}

Having installed Duplicati last Friday in class, I noticed that it didn't have access to my Documents folder. After doing some reading, I think that is just a permissions thing. I will probabyl have to rewrite some of the script so that the PoC directory should be stored in the User folder.

\textbf{Objective:} Rewrite scripts so that they now work when the PoC folder is stored in the User folder and not the Documents folder.

\textbf{Action:} In the respective R Scripts, changed the \begin{verbatim}
    folder <- "~/Documents/PoC/data/corpus_output/body"
\end{verbatim}

to \begin{verbatim}
    folder <- "~/PoC/data/corpus_output/body"
\end{verbatim}

and changed \begin{verbatim}
    write.csv(d, file=paste0("~/Documents/PoC/data/wordfreq/body/
    bodywordfrequency-", Sys.time(), ".csv"), row.names=FALSE)
\end{verbatim} to \begin{verbatim}
    write.csv(d, file=paste0("~/PoC/data/wordfreq/body/
    bodywordfrequency-", Sys.time(), ".csv"), row.names=FALSE)
\end{verbatim}

Moved the PoC folder to my home User folder than ran go.sh.

\textbf{Error:} None.

\textbf{Result:} Success!

\textbf{29/10/19 - 4:51pm}

I have reconfigured the backup with Duplicati to back up Users/PoC. There no longer seems to be an error and a successful backup has been performed.

\newpage
\subsection{Proof of Concept Final Scripts}

\subsubsection{go.sh Bash Script}

\begin{verbatim}
#!/bin/bash

bash extractbody.sh
bash extracttitles.sh
bash extracttitlessubtitles.sh
\end{verbatim}

\subsubsection{extractbody.sh Bash Script}

\begin{verbatim}
#!/bin/bash

# Create output directories if needed
mkdir -p ~/corpus-analysis/data/corpus_output/
mkdir -p ~/corpus-analysis/data/corpus_output/body
mkdir -p ~/corpus-analysis/data/wordfreq/
mkdir -p ~/corpus-analysis/data/wordfreq/body

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do tail -n +6 $filename > ../corpus_output/body/body-${filename}
done

# Go back to the bin folder to run R script
cd ../..
cd bin
Rscript wordfreq_body.R
\end{verbatim}

\subsubsection{extracttitles.sh Bash Script}

\begin{verbatim}
#!/bin/bash

# Create output directories if needed
mkdir -p ~/corpus-analysis/data/corpus_output/
mkdir -p ~/corpus-analysis/data/corpus_output/titles
mkdir -p ~/corpus-analysis/data/wordfreq/
mkdir -p ~/corpus-analysis/data/wordfreq/titles

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do head -n 2 $filename | tail -n 1 > ../corpus_output/titles/title-${filename}
done

# Go back to the bin folder to run R script
cd ../..
cd bin
Rscript wordfreq_titles.R
\end{verbatim}

\subsubsection{extracttitlessubtitles.sh Bash Script}

\begin{verbatim}
#!/bin/bash

# Create output directories if needed
mkdir -p ~/corpus-analysis/data/corpus_output/
mkdir -p ~/corpus-analysis/data/corpus_output/titlessubtitles
mkdir -p ~/corpus-analysis/data/wordfreq/
mkdir -p ~/corpus-analysis/data/wordfreq/titlessubtitles

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do head -n 3 $filename | tail -n 2 > 
../corpus_output/titlessubtitles/titlesubtitles-${filename}
done

# Go back to the bin folder to run R script
cd ../..
cd bin
Rscript wordfreq_titlessubtitles.R
\end{verbatim}

\subsubsection{delete.sh Bash Script}

\begin{verbatim}
#!/bin/bash

# Changes directory to corpus_output/body directory
cd ..
cd data/corpus_output/body/

# Delete .txt files
rm *.txt

# Changes directory to corpus_output/titles directory
cd ../titles

# Delete .txt files
rm *.txt

# Changes directory to corpus_output/titlessubtitles directory
cd ../titlessubtitles/

# Delete .txt files
rm *.txt

# Changes directory to bin directory
cd ../../../bin/
\end{verbatim}

\subsubsection{wordfreq\_body.R Script}

\begin{verbatim}
# Package and library required for this work
if(!require(tm)){
install.packages("tm",repos = "http://cran.us.r-project.org")
library(tm)
}
# The file path to where the .txt files are stored
folder <- "~/corpus-analysis/data/corpus_output/body"
# Lists the files in the path
filelist <- list.files(path=folder, pattern="*.txt")
# Creates a path to all .txt files in path folder
filelist <- paste(folder, "/", filelist, sep="")
# Reads text from .txt files
readtext <- lapply(filelist, FUN = readLines)
# Collapses elements into one element
corpus <- lapply(readtext, FUN = paste, collapse=" ")
# Convert corpus to something tm package can use
VCorpus <- Corpus(VectorSource(corpus))
# Using a new corpus name 'VCorpus_clean to keep the original VCorpus 
object untouched
# Strip whitespace from corpus
VCorpus_clean <- tm_map(VCorpus, stripWhitespace)
# Convert corpus to lower case
VCorpus_clean <- tm_map(VCorpus_clean, content_transformer(tolower))
# Remove German stop words
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, stopwords("german"))
# Remove punctuation
VCorpus_clean <- tm_map(VCorpus_clean, removePunctuation)
# Remove numbers
VCorpus_clean <- tm_map(VCorpus_clean, removeNumbers)
# Remove """ and "—"from corpus
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, c(""","—"))
# Create Term-Document Matrix
dtm <- TermDocumentMatrix(VCorpus_clean)
# To get the list of frequency of words
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
# Output dataframe as a .csv file
write.csv(d, file=paste0("~/corpus-analysis/data/wordfreq/body/
bodywordfrequency-", Sys.time(), ".csv"), row.names=FALSE)
\end{verbatim}

\subsubsection{wordfreq\_titles.R Script}

\begin{verbatim}
# Package and library required for this work
if(!require(tm)){
install.packages("tm",repos = "http://cran.us.r-project.org")
library(tm)
}
# The file path to where the .txt files are stored
folder <- "~/corpus-analysis/data/corpus_output/titles"
# Lists the files in the path
filelist <- list.files(path=folder, pattern="*.txt")
# Creates a path to all .txt files in path folder
filelist <- paste(folder, "/", filelist, sep="")
# Reads text from .txt files
readtext <- lapply(filelist, FUN = readLines)
# Collapses elements into one element
corpus <- lapply(readtext, FUN = paste, collapse=" ")
# Convert corpus to something tm package can use
VCorpus <- Corpus(VectorSource(corpus))
# Using a new corpus name 'VCorpus_clean to keep the original 
VCorpus object untouched
# Strip whitespace from corpus
VCorpus_clean <- tm_map(VCorpus, stripWhitespace)
# Convert corpus to lower case
VCorpus_clean <- tm_map(VCorpus_clean, content_transformer(tolower))
# Remove German stop words
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, stopwords("german"))
# Remove punctuation
VCorpus_clean <- tm_map(VCorpus_clean, removePunctuation)
# Remove numbers
VCorpus_clean <- tm_map(VCorpus_clean, removeNumbers)
# Remove """ and "—"from corpus
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, c("","—"))
","—"))
# Create Term-Document Matrix
dtm <- TermDocumentMatrix(VCorpus_clean)
# To get the list of frequency of words
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
# Output dataframe as a .csv file
write.csv(d, file=paste0("~/corpus-analysis/data/wordfreq/titles/
titlewordfrequency-", Sys.time(), ".csv"), row.names=FALSE)
\end{verbatim}

\subsubsection{wordfreq\_titlessubtitles.R Script}

\begin{verbatim}
# Package and library required for this work
if(!require(tm)){
install.packages("tm",repos = "http://cran.us.r-project.org")
library(tm)
}
# The file path to where the .txt files are stored
folder <- "~/corpus-analysis/data/corpus_output/titlessubtitles"
# Lists the files in the path
filelist <- list.files(path=folder, pattern="*.txt")
# Creates a path to all .txt files in path folder
filelist <- paste(folder, "/", filelist, sep="")
# Reads text from .txt files
readtext <- lapply(filelist, FUN = readLines)
# Collapses elements into one element
corpus <- lapply(readtext, FUN = paste, collapse=" ")
# Convert corpus to something tm package can use
VCorpus <- Corpus(VectorSource(corpus))
# Using a new corpus name 'VCorpus_clean to keep the original 
VCorpus object untouched
# Strip whitespace from corpus
VCorpus_clean <- tm_map(VCorpus, stripWhitespace)
# Convert corpus to lower case
VCorpus_clean <- tm_map(VCorpus_clean, content_transformer(tolower))
# Remove German stop words
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, stopwords("german"))
# Remove punctuation
VCorpus_clean <- tm_map(VCorpus_clean, removePunctuation)
# Remove numbers
VCorpus_clean <- tm_map(VCorpus_clean, removeNumbers)
# Remove """ and "—"from corpus
VCorpus_clean <- tm_map(VCorpus_clean, removeWords, c(""","—"))
# Create Term-Document Matrix
dtm <- TermDocumentMatrix(VCorpus_clean)
# To get the list of frequency of words
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
# Output dataframe as a .csv file
write.csv(d, file=paste0("~/corpus-analysis/data/wordfreq/
titlessubtitles/titlesubtitlewordfrequency-", 
Sys.time(), ".csv"), row.names=FALSE)

\end{verbatim}

\textbf{1/11/19 - 7:21pm}

Sometimes when running the R Script, it is unable to load the dependency "slam".

\newpage
\subsection{Bugs in Proof of Concept}

\textbf{29/10/19 - 4:21pm}

When running the R Scripts, these messages appear: \begin{verbatim}
    Warning message:
In tm_map.SimpleCorpus(VCorpus, stripWhitespace) :
  transformation drops documents
Warning message:
In tm_map.SimpleCorpus(VCorpus_clean, content_transformer(tolower)) :
  transformation drops documents
Warning message:
In tm_map.SimpleCorpus(VCorpus_clean, removeWords, stopwords("german")) :
  transformation drops documents
Warning message:
In tm_map.SimpleCorpus(VCorpus_clean, removePunctuation) :
  transformation drops documents
Warning message:
In tm_map.SimpleCorpus(VCorpus_clean, removeWords, c(""", "—")) :
  transformation drops documents
\end{verbatim}

Having checked the documents, it doesn't seem to be losing any terms. So I'm not exactly sure by what this means.

\textbf{29/10/19 - 4:22pm}

For some reason, the long - and the upside down German " do not get removed from the corpus.

\newpage
\subsection{Errors in Proof of Concept}

\textbf{10/10/19 - 11:40pm}

\textbf{Objective:} See if I can run Voyant Tools on my computer.

\textbf{Action:}

\begin{itemize}
    \item Downloaded Voyant Tools .zip file from Voyant Server website
    \item Unpackaged .zip file
    \item Run VoyantServer.jar
\end{itemize}

\textbf{Error:} VoyantServer.jar would not open

\textbf{Result:} VoyantServer.jar wouldn't run because it needs to install Java Development Kit to run.

\textbf{17/10/19 - 11:09pm}

Now to see if I can create a new text file with all titles in it.

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1
    cat $filename >> articletitles.txt
    done
\end{verbatim}

\textbf{Error:} Yes, and error! It didn't work exactly as planned. 

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files. I think this is because the \begin{verbatim}
    cat $filename >> articletitles.txt
\end{verbatim} command wasn't piped the the earlier \begin{verbatim}
    do head -n 2 $filename | tail -n 1
\end{verbatim} command.

\textbf{17/10/19 - 11:16pm}

\textbf{Objective:} Create a new text file filled with all the titles of the articles.

\textbf{Action:} Used the following code: \begin{verbatim}
    for filename in *.txt
    do head -n 2 $filename | tail -n 1 | cat $filename >> articletitles.txt
    done\end{verbatim}

\textbf{Error:} Same error as before...

\textbf{Result:} A new file 'articletitles.txt' was created in the directory. It seems to have created a .txt file with ALL the text from all the .txt files.

\textbf{23/10/19 - 3:13pm}

Now that I have Voyant Server running on my machine, I want to see if I can change the Voyant Server settings so that it will automatically open a directory and use .txt files wherein to make a new corpus.

\textbf{Objective:} Change serversettings.txt in the Voyant Server data folder to point to the corpus\_output/body/ directory to open up the body .txt files.

\textbf{Action:} Changed \begin{verbatim}
    uri_path = 
\end{verbatim}

to \begin{verbatim}
    uri_path = /?input=/Users/janakin/Documents/PoC/data/corpus_output/body/
\end{verbatim}

\textbf{Error:} Yes.

\textbf{Result:} Voyant server opened up an analysis window, but only analysed the path itself, i.e. it analysed janakin, Documents, PoC, data, corpus, output and body.

\textbf{23/10/19 - 9:54pm}

Now to get R to read the text from the .txt files.

\textbf{Objective:} Get R to read the text from the .txt files.

\textbf{Action:} Used the code \begin{verbatim}
    readtext <- lapply(filelist, FUN = readLines)
\end{verbatim}

\textbf{Error:} Some. \begin{verbatim}
    Warning messages:
1: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-14_Australienqualifiziert.txt'
2: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-14_Kurzberichtet.txt'
3: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-15_DiefalschenGarantienderchilenischenJunta.txt'
4: In FUN(X[[i]], ...) :
  incomplete final line found on '/Users/janakin/Documents/PoC/data/
corpus_output/body/body-1973-11-16_Bulgartonqualifiziert.txt'
\end{verbatim}

However, the tutorial I read said that this would happen because some of the .txt files may have not finished with an ENTER command.

\textbf{23/10/19 - 10:25pm}

\textbf{Objective:} Create new corpus object and remove whitespace.

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus, stripWhitespace)
\end{verbatim}

\textbf{Error:} I think? \begin{verbatim}
    Warning message:
In tm_map.SimpleCorpus(VCorpus, stripWhitespace) :
  transformation drops documents
\end{verbatim}

\textbf{Result:} Despite the warning message, it still stripped whitespace.

\textbf{23/10/19 - 10:29pm}

\textbf{Objective:} Convert corpus to lower case

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus, content_transformer(tolower))
\end{verbatim}

\textbf{Error:} Same thing as the white space \begin{verbatim}
    Warning message:
In tm_map.SimpleCorpus(VCorpus, content_transformer(tolower)) :
  transformation drops documents
\end{verbatim}

\textbf{Result:} Success despite warning message.

\textbf{23/10/19 - 10:34pm}

\textbf{Objective:} Remove German stop words

\textbf{Action:} Used the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus_clean, removeWords, stopwords("german"))
\end{verbatim}

\textbf{Error:} Same as the past two code lines.

\textbf{Result:} Success despite warning message.

\textbf{23/10/19 - 11:00pm}

\textbf{Objective:} Remove """ (The upside down German version) from corpus

\textbf{Action:} Entered the code \begin{verbatim}
    VCorpus_clean <- tm_map(VCorpus_clean, removeWords, c("""))
\end{verbatim} into the script.

\textbf{Error:} It didn't remove the """ (The upside down German version)

\textbf{Result:} It didn't remove the """ (The upside down German version). I'll just move on for now and see if I can fix it later.

\textbf{24/10/19 - 10:59pm}

Now for the big test. I want to see if I can get my bash script to run my R script as well.

\textbf{Objective:} Modify extractbody.sh to run the R script that creates a .csv file from the word frequency found in a corpus.

\textbf{Action:} \begin{itemize}
    \item Copied textualanalysis.R
    \item Pasted textualanalysis.R in Poc/bin/
    \item Renamed textualanalysis to wordfreq\_body.R
    \item Rewrote extractbody.sh to \begin{verbatim}
        #!/bin/bash

# Changes directory to corpus_input directory
cd ..
cd data/corpus_input/

# Loop to extract body text and output to new .txt files in corpus_output
for filename in *.txt
do tail -n +6 $filename > ../corpus_output/body/body-${filename}
done

# Go back to the bin folder to run R script
cd ../..
cd bin
Rscript wordfreq_body.R
    \end{verbatim}
    \item Ran bash extractbody.sh
\end{itemize}

\textbf{Error:} Yes

\textbf{Result:} This was the error message in Terminal \begin{verbatim}
    Error in file(con, "r") : cannot open the connection
\end{verbatim}

I will revisit this later.

\textbf{18/10/19 - 12:30pm}

After thinking I had finished my PoC user testing I had fellow students Sophie Wallace and Tom Duloy to download and test my bash and R scripts. 

\textbf{Objective:} Test go.sh bash script.

\textbf{Action:} \begin{itemize}
    \item Downloaded PoC.zip from my Github repository.
    \item Unzipped PoC.zip to Documents folder.
    \item Ran go.bash from the bin directory in Terminal.
\end{itemize}

\textbf{Error:} Yes.

\textbf{Result:} This was the Terminal error message: \begin{verbatim}
    Error in contrib.url(repos, "source") : 
  trying to use CRAN without setting a mirror
Calls: install.packages -> contrib.url
In addition: Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘tm’
Execution halted
\end{verbatim}

\newpage
\section{Misc Notes on FOAR705}

\textbf{4/6/19 - 11:06am}

I have decided to keep using this Learning Journal for the rest of the unit. I will still upload them to Cloudstor on a weekly basis for consistency. I have chosen to do this so that I will have a more comprehensive journal that I can refer back to later in the unit.

Another general note. I still think my OSP is too vague. I don't know how to make it more specific. Hopefully I will get some guidance about it today in class. I really don't have time to go to the consultation hours due to the immense workload that I have from this unit, on top with all my other units.

\end{document}
